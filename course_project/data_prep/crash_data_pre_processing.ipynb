{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "839a9070",
   "metadata": {},
   "source": [
    "# Crash Data Big Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "420616eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lt/90cmmvp91vn9qgdwpxyyb6l40000gn/T/ipykernel_81449/3423538044.py:9: DtypeWarning: Columns (32,84,100,102) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_0 = pd.read_csv(\"../data/CT_crashes/export_5673_0.csv\", encoding=\"cp1252\", skiprows=1)\n",
      "/var/folders/lt/90cmmvp91vn9qgdwpxyyb6l40000gn/T/ipykernel_81449/3423538044.py:10: DtypeWarning: Columns (7,76,81,88,98,99) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_1 = pd.read_csv(\"../data/CT_crashes/export_5673_1.csv\", encoding=\"cp1252\", skiprows=1)\n"
     ]
    }
   ],
   "source": [
    "# Package Upload\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Data Upload\n",
    "\n",
    "## Three files (NOTE THAT THEY DO NOT USE UTF-8 ENCODING! ALSO: THE FIRST ROW IS JUST A HYPERLINK!)\n",
    "df_0 = pd.read_csv(\"../data/CT_crashes/export_5673_0.csv\", encoding=\"cp1252\", skiprows=1)\n",
    "df_1 = pd.read_csv(\"../data/CT_crashes/export_5673_1.csv\", encoding=\"cp1252\", skiprows=1)\n",
    "df_2 = pd.read_csv(\"../data/CT_crashes/export_5673_2.csv\", encoding=\"cp1252\", skiprows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef132519",
   "metadata": {},
   "source": [
    "## Individual File Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6f65235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep Columns Lists:\n",
    "\n",
    "## dataset 0:\n",
    "\n",
    "keep_columns_0 = [\n",
    "        \"CrashId\",\n",
    "        \"Fatal Case Status\",\n",
    "        \"Latitude\",\n",
    "        \"Longitude\",\n",
    "        \"Date of Crash\",\n",
    "        \"Day of the Week\",\n",
    "        \"Day of the Week Text Format\",\n",
    "        \"Hour of the Day\",\n",
    "        \"Most Severe Injury\",\n",
    "        \"Number Of Motor Vehicles\",\n",
    "        \"Number Of Non-Motorist\",\n",
    "        \"Route Class\", \"Route Class Text Format\",\n",
    "        \"First Harmful Event\",\n",
    "        \"Manner of Crash / Collision Impact\",\n",
    "        \"Location of First Harmful Event\",\n",
    "        \"Weather Condition\", \"Weather Condition Text Format\",\n",
    "        \"Light Condition\", \"Light Condition Text Format\",\n",
    "        \"Road Surface Condition\", \"Road Surface Condition Text Format\",\n",
    "        \"Crash Specific Location\", \"Crash Specific Location Text Format\",\n",
    "        \"Type of Intersection\", \"Type of Intersection Text Format\"\n",
    "    ]\n",
    "\n",
    "## dataset 1:\n",
    "\n",
    "keep_columns_1 = [\n",
    "        \"CrashId\", \"VehicleId\", \"Vehicle Unit Type\", \"Make\", \n",
    "        \"Vehicle Model Year\", \"Model\", \"Vehicle Color\", \n",
    "        \"Most Harmful Event\", \"Most Harmful Event Text Format\",\n",
    "        \"Vehicle Manuever/Action\", \"Vehicle Manuever/Action Text Format\",\n",
    "        \"Roadway Grade\", \"Extent of Damage\", \"Extent of Damage Text Format\",\n",
    "        \"Body Type\", \"Body Type Text Format\"\n",
    "    ]\n",
    "\n",
    "## dataset 2:\n",
    "\n",
    "keep_columns_2 = [\n",
    "        \"CrashId\", \"Age\", \"Gender\", \"Gender Text Format\", \"State\", \"Postal Code\",\n",
    "        \"Person Type\", \"Person Type Text Format\", \"Injury Status\",\n",
    "        \"Air Bag Status\", \"Air Bag Status Text Format\",\n",
    "        \"Speeding Related\", \"Speeding Related Text Format\"\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d304d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precursor functions for Data Cleaning:\n",
    "\n",
    "## IDENTIFY NUMERIC, TEXT FIELDS TO EXPORT TEXT VERSION TO DECODING FILE FOR STORAGE PRESERVATION\n",
    "def extract_decodings(df, decoding_records):\n",
    "    \"\"\"\n",
    "    Extract unique code â†’ text mappings from columns ending in ' Text Format'\n",
    "    and append them to decoding_records (a list of dicts).\n",
    "    \"\"\"\n",
    "    text_cols = [c for c in df.columns if c.endswith(\" Text Format\")]\n",
    "\n",
    "    for text_col in text_cols:\n",
    "        code_col = text_col.replace(\" Text Format\", \"\")\n",
    "\n",
    "        if code_col not in df.columns:\n",
    "            continue\n",
    "\n",
    "        pairs = (\n",
    "            df[[code_col, text_col]]\n",
    "            .dropna()\n",
    "            .drop_duplicates()\n",
    "        )\n",
    "\n",
    "        for _, row in pairs.iterrows():\n",
    "            decoding_records.append({\n",
    "                \"variable\": code_col,\n",
    "                \"code\": row[code_col],\n",
    "                \"text\": row[text_col]\n",
    "            })\n",
    "\n",
    "    # Drop text columns after extraction\n",
    "    df = df.drop(columns=text_cols)\n",
    "\n",
    "    return df\n",
    "\n",
    "## TAKE AN INITIAL DATAFRAME (0,1,2) AND USE ITS KEEP_COLUMNS TO REMOVE UNNECESSARY INFO\n",
    "def reduce_single_df(df, keep_columns, decoding_records):\n",
    "    \"\"\"\n",
    "    Reduce a single dataframe:\n",
    "    - Keep only desired columns\n",
    "    - Extract decoding info\n",
    "    - Remove 'Text Format' columns\n",
    "    \"\"\"\n",
    "\n",
    "    # Keep only requested columns (safe intersection)\n",
    "    df = df.loc[:, df.columns.intersection(keep_columns)]\n",
    "\n",
    "    # Extract and remove text-format columns\n",
    "    df = extract_decodings(df, decoding_records)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "270a77ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA CLEANING EXECUTION FUNCTION\n",
    "def clean_and_merge_crash_data(\n",
    "    dfs,\n",
    "    keep_columns_list,\n",
    "    output_dir=\"../data/clean_crash_data\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    dfs : list[pd.DataFrame]\n",
    "        List of crash dataframes\n",
    "    keep_columns_list : list[list[str]]\n",
    "        Matching list of keep-columns for each df\n",
    "    \"\"\"\n",
    "\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    decoding_records = []\n",
    "    cleaned_dfs = []\n",
    "\n",
    "    for df, keep_cols in zip(dfs, keep_columns_list):\n",
    "        reduced = reduce_single_df(df, keep_cols, decoding_records)\n",
    "        cleaned_dfs.append(reduced)\n",
    "\n",
    "    # ---- Merge datasets ----\n",
    "    merged = pd.concat(cleaned_dfs, axis=0, ignore_index=True)\n",
    "\n",
    "    # ---- Remove duplicate columns by value ----\n",
    "    merged = merged.T.drop_duplicates().T\n",
    "\n",
    "    # ---- Save clean crash data ----\n",
    "    clean_path = Path(output_dir) / \"clean_crash_data.csv\"\n",
    "    merged.to_csv(clean_path, index=False)\n",
    "\n",
    "    # ---- Build and save decoding table ----\n",
    "    if decoding_records:\n",
    "        decoding_df = (\n",
    "            pd.DataFrame(decoding_records)\n",
    "            .drop_duplicates()\n",
    "            .sort_values([\"variable\", \"code\"])\n",
    "        )\n",
    "    else:\n",
    "        decoding_df = pd.DataFrame(columns=[\"variable\", \"code\", \"text\"])\n",
    "\n",
    "    decoding_path = Path(output_dir) / \"decoding_table.csv\"\n",
    "    decoding_df.to_csv(decoding_path, index=False)\n",
    "\n",
    "    return merged, decoding_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b3fc4a",
   "metadata": {},
   "source": [
    "## Merging Files Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d532881b",
   "metadata": {},
   "source": [
    "### Reducing file size significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cadbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df, decoding_df = clean_and_merge_crash_data(\n",
    "    dfs=[df_0, df_1, df_2],\n",
    "    keep_columns_list=[keep_columns_0, keep_columns_1, keep_columns_2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b827ec54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
      "\n",
      "[1612723 rows x 0 columns]\n"
     ]
    }
   ],
   "source": [
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734e1cb5",
   "metadata": {},
   "source": [
    "## Building Decoding Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70774d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Harmful Event\n",
    "# Most severe injury\n",
    "# Manner of Crash / Collision Impact\n",
    "# Location of First Harmful Evenet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eecspring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
