---
title: "STAT 5605 Homework 3"
subtitle: "February 19, 2026"
author: "Jack Bienvenue"
output:
  bookdown::pdf_document2:
    includes:
      in_header: preamble.tex
    latex_engine: pdflatex
    keep_tex: true 
header-includes:
  - \usepackage{titling}
  - \setlength{\droptitle}{1em} 
fontsize: 12pt
---

```{r, echo=FALSE}
invisible(lapply(c("ggplot2", "dplyr", "tidyr"), 
  function(pkg) suppressPackageStartupMessages(library(pkg, 
                                                character.only = TRUE))))
```

# Problem 1

A large, national grocery retailer tracks productivity and costs of its facilities closely. A subset of the data, HW3PR1.txt, was obtained from a single distribution center for a one-year period. The variables included are $X_1=$ the number of cases shipped, $X_2=$ the indirect costs of the total labor hours as a percentage, and $X_3=1$ if the week has a holiday and 0 otherwise. The response variable $Y$ is the total labor hours.

```{r}
HW3PR1 <- read.table("./HW3PR1.txt", header=TRUE)
str(HW3PR1)
nrow(HW3PR1)
PR1 <- lm(formula = Y ~ X1+X2+X3, data=HW3PR1)
summary(PR1)
new_x_value <- data.frame(X1 = 400000, X2=7.2, X3=0)
predicted_value1 <- predict(object = PR1, newdata = new_x_value, 
                            interval="confidence")
print(predicted_value1)
predicted_value2 <- predict(object = PR1, newdata = new_x_value, 
                            interval="prediction")
print(predicted_value2)
```

## (a) 

Create a multivariate scatter plot matrix for $X_1$,$X_2$, $X_3$, and $Y$. What information does this plot provide?

```{r}
HW3PR1a <- select(HW3PR1, X1,X2,X3,Y)
pairs(HW3PR1a)
```

\begin{bluebox}
a
\end{bluebox}

## (b)

The cases in HW3PR1.txt are given in consecutive weeks.  Prepare a time plot for each predictor variable as well as the response variable $Y$. What do these plots show?

```{r}
# Data prep for nice time series plot:
HW3PR1_long <- HW3PR1 %>% mutate(Week = case) %>%
  select(Week, Y, X1, X2, X3) %>%
  pivot_longer( cols = c(Y, X1, X2, X3),
                 names_to = "Variable",
                   values_to = "Value") 
HW3PR1_long$Variable <- recode(HW3PR1_long$Variable,
  Y  = "Total Labor Hours (Y)",
  X1 = "Cases Shipped (X1)",
  X2 = "Indirect Labor % (X2)",
  X3 = "Holiday Indicator (X3)"
)

# 2x2 Scatterplot of the time series for the variables:
ggplot(HW3PR1_long, aes(x = Week, y = Value)) +
  geom_point(alpha = 0.6, color = "gray40") +
  geom_smooth(
    method = "gam",
    formula = y ~ s(x, bs = "cs"),  # cubic spline fit 
    se = TRUE,
    color = "#376199",
    linewidth = 1
  ) +
  
  facet_wrap(~ Variable, ncol = 2, scales = "free_y") +
  
  labs(
    title = "Weekly Time Series for Grocery Store Data Variables",
    x = "Week",
    y = "Value",
    caption = "Cubic Spline Fit Used for Time Series | Jack Bienvenue 2026"
  ) +
  
  theme_minimal(base_size = 13) +
  theme(
  plot.title = element_text(
    face = "bold",
    hjust = 0.5),
  axis.title = element_text(
    face = "bold"),
  plot.caption = element_text(
    color = "gray60"))
```

\begin{bluebox}
In this part of the problem, we would like to assess whether there is clear seasonality in the trends of any of the predictors or the response variable. We should realize that, if we were to create predictive models or make inferences based upon this data, that inferences or predictions may be inaccurate because of time-dependence issues. 

These plots provide several insights into how time dependence plays a role in this data. For the top-left plot, we observe a potentially meaningful time series signal of cases shipped increasing on average towards the end of the year, potentially associated with the holiday season assuming that Week 0 indicates the beginning of the calendar year. Following this assumption, we see shipments gradually decrease on average as the winter and spring progress, bottoming in the summer and increasing again on average as the fall progresses. 

The signal for indirect labor percentages is less clear, but suggests that the percentage may gradually increase on average as the year progresses.

The holiday indicator's plot is not helpful in its own right, but provides clarity on where the holidays occur during the year. We can observe that $12\%$ of the weeks in the year are considered holidays.

When reviewing the outcome variable, we can see that the total labor hours are on average relatively stagnant throughout the year, not exhibiting obvious seasonality. However, in combination with the holiday indicator plot, we can observe that in this data set, the highest total labor hours week coincide with the holiday weeks. 

We should note when reviewing these time series plots that we are observing the time series over a single year and that the trends we observe over a single year may not be fully representative of the true time series for these variables. 
\end{bluebox}


## (c) 

Test whether there is a regression relation. State the hypotheses and conclusion. What does your test result imply about regression coefficients (slopes) associated with $X_1$, $X_2$, and $X_3$?  What is the p-value of the test?

```{r}

```

\begin{bluebox}
Before performing our test, let's set the hypotheses for our testing:

$$
a
$$

$$
a
$$

Our regression output provides...
\end{bluebox}

## (d) 

Calculate the coefficient multiple determination $R^2$. How is this measure interpreted here.

```{r}

```

\begin{bluebox}
a
\end{bluebox}

## (e) 

For the data, HW3PR1.txt, on which the regression fit is based, would you consider a shipment of 400,000 cases with an indirect percentage of 7.2 on a non-holiday week to be within the scope of the model? What about a shipment of 400,000 cases with an indirect percentage of 9.9 on a non-holiday week? Support your answers by preparing or referring to a relevant plot.

\begin{bluebox}
a
\end{bluebox}

## (f)  

Assume that multiple regression model for three predictor variables with independent normal error terms is appropriate. A new shipment is to be received with $x_{h1}=292,087$, $x_{h2}=7.77$, and $x_{h3}=0$.

\begin{bluebox}
a
\end{bluebox}

### (f1) 

Obtain a 95\% confidence interval for the expected total labor hours for this shipment.
  
\begin{bluebox}
a
\end{bluebox}

### (f2) 

Compute a 95\% prediction interval for the total labor hours for this new shipment.

\begin{bluebox}
a
\end{bluebox}

# Problem 2

An analyst wanted to fit a regression model $Y_i=\beta_0+\beta_1X_{i1} + \beta_{2} X_{i2} + \beta_3 X_{i3} + \epsilon_i$, $i=1,2,\dots,n$, by the method of least squares when it is known that $\beta_2=5$. How can the analyst obtain the desired fit by using a multiple regression computer program? What if it is known that $\beta_3=0$?

\begin{bluebox}
To generate a least squares fit for MLR where one coefficient (e.g., $\beta_2 = 0$) is already known, ...

To generate a least squares fit for 3-predictor MLR where two population coefficients (e.g., $\beta_2, \beta_3$) are known, we can HOW DO YOU DO THAT???????

\end{bluebox}

# Problem 3 

Consider the multiple linear regression model:   $Y_i=\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i1} X_{i2} + \epsilon_i$, where $\epsilon_i \sim N(0,\sigma^2)$, $i=1,2,\dots,n$. Which of the following statements would be correct?  Provide a necessary explanation.

## (a)  

The number of predictors is 2.

\begin{bluebox}
\textbf{INCORRECT*} - We have three predictors that we are using for our model: $X_{i1}$,$X_{i2}$, and $X_{i1}X_{i2}$.

However, if we are considering predictors to be unique features, we have two unique features here: $X_1$ and $X_2$. We also have an interaction term $X_1X_2$ formed from $X_1$ and $X_2$. Under this circumstance, we could claim that this statement is true.
\end{bluebox}

## (b)  

The number of regressors is 3.
  
\begin{bluebox}
\textbf{INCORRECT*} - We are using four regressors here, $1, X_{i1}, X_{i2}, \text{ and } X_{i1}X_{i2}$, multiplied by $\beta_0, \beta_1, \beta_2, \beta_3$, respectively. 

If we do not consider the intercept term to be a regressor, then we could say we have three distinct regressors for $Y$: $\beta_1X_{i1}$, $\beta_2 X_{i2}$, $\beta_3 X_{i1}X_{i2}$.
\end{bluebox}

## (c)  

The number of model parameters is 3.

\begin{bluebox}
\textbf{INCORRECT} - We have five distinct parameters for our model: $\beta_0, \beta_1, \beta_2, \beta_3, \sigma^2$.
\end{bluebox}

## (d)  

The number of model parameters is 4.

\begin{bluebox}
\textbf{INCORRECT} - We have five distinct parameters for our model: $\beta_0, \beta_1, \beta_2, \beta_3, \sigma^2$.
\end{bluebox}

## (e)  

The number of model parameters is 5.

\begin{bluebox}
\textbf{CORRECT} - the model parameters are $\beta_0, \beta_1, \beta_2, \beta_3$ and the variance of the error term, $\sigma^2$. This makes for a total of $5$ parameters in the model. 
\end{bluebox}

## (f)  

The number of model parameters is $\infty$.

\begin{bluebox}
\textbf{INCORRECT} - our model has closed form $Y_i=\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i1} X_{i2} + \epsilon_i$, where $\epsilon_i \sim N(0,\sigma^2)$, $i=1,2,\dots,n$ with $5$ uniquely identifiable parameters as we observed for the previous statement. The number of model parameters therefore is finite and equal to $5$, the number of model parameters is not $\infty$.
\end{bluebox}

## (g)  

None of the above.

\begin{bluebox}
\textbf{INCORRECT} - We found at least \textbf{part e} to be true.
\end{bluebox}


# Problem 4 

An analysis is performed to study the relationship between three explanatory variables, $X_1$, $X_2$ and $X_3$, and a response variable $Y$.  Consider the following model:
$$
Y=\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3+ \epsilon
$$
where $\epsilon \sim N(0,\sigma^2)$. The observed data is denoted by
 $\{(Y_i,X_{1i},X_{2i},X_{3i}),\; i=1,2,\dots,n\}$ and we further assume that
the $Y_i$'s are independent. We fit the above model to this data set.
The resulting ANOVA table is given below and  the coefficient of determination is 0.637538.

\begin{center}
{\bf The  ANOVA Table}

\begin{tabular}{lcrrrr} \hline
\multicolumn{6}{c}{Analysis of Variance} \\ \hline
Source  & DF       & Sum of Square  & Mean Square &  F Stat & Prob $>$ F \\ \hline
Model   & $\ast$   &   $\ast$       &   $\ast$    &  $\ast$ &   $\ast$ \\
Error   &  117      &  17.90761    &  0.15306    &         &   \\
C Total & $\ast$   &   $\ast$     &              &         & \\ \hline
\end{tabular}
\end{center}

Answer the following questions:

## (i) 

Fill in the missing values (denoted by ``$\ast$'') in the ANOVA table.

```{r}
prob4_1_quantity <- pf(65.72, 3, 117, lower.tail = FALSE)
print(prob4_1_quantity)
```

\begin{bluebox}
In this problem, we are given a small amount of information, but we should be able to work backwards to get all of the quantities we need to fill our table.

Besides the information about the error degrees of freedom, associated SS, and MS, we know that the coefficient of determination $R^2 = 0.627538 = \frac{SSR}{SSTO} = 1 - \frac{SSE}{SSTO}.$ Right away, we can actually find $SSTO$ and $SSR$:

$$
\frac{SSE}{SSTO} = 1 - 0.627538,
$$
Leading to:

$$
\frac{17.90761}{SSTO} = 0.372462 \implies SSTO = \frac{17.90761}{0.372462} \approx 48.07903.
$$

Therefore our $SSR$ is:

$$
SSR = 0.627538 \times 48.07903 \approx 30.17142.
$$

Moving on, we know in MLR that the degrees of freedom for the error term is $n-p$, with $p$ representing the number of parameters to be estimated. Here, we have four parameters to be estimated, so using our given error degrees of freedom $117$, we can find that our sample size is $n=121$. To find the model and total degrees of freedom, we recognize that the MLR model degrees of freedom is equal to the number of predictors, $3$, and the corrected total degrees of freedom is equal to $n-1$, so here the corrected total degrees of freedom would be $120$.

To find the relevant mean square error for the regression model, we simply need to divide the SSR by its corresponding degrees of freedom, $\frac{30.17142}{3} = 10.05714.$

Finally, we can round out our table by finding the F Statistic and its corresponding p-value by first identifying the F Statistic's value, $F = \frac{MSR}{MSE} = \frac{10.05714}{0.15306} \approx 65.70717 $. This means that we can find the p-value through calculating the probability $P(F_{3,117} > 65.72)$. This probability will be extremely close to zero. Comfortably we can claim that the p-value would end up being $\ll 0.0001$.

So, our completed table is:

\begin{center}
{\bf The  ANOVA Table}

\begin{tabular}{lcrrrr} \hline
\multicolumn{6}{c}{Analysis of Variance} \\ \hline
Source  & DF       & Sum of Square  & Mean Square &  F Stat & Prob $>$ F \\ \hline
Model   & 3   &   30.17142       &   10.05714    &  65.72 &   $\ll 0.0001$\\
Error   &  117      &  17.90761    &  0.15306    &         &   \\
C Total & 120   &   48.07903     &              &         & \\ \hline
\end{tabular}
\end{center}
\end{bluebox}

## (ii) 

State the null and alternative hypotheses ($H_0$ and $H_1$) for the $F$ test in the ANOVA table.

\begin{bluebox}
For the type of F test employed by our table setup, we are testing whether the linear regression model overall explains any variation in Y. Therefore we are testing the hypothesis:

$$
H_0: \beta_1 = \beta_2 = \beta_3 = 0, \text{ against}
$$

$$
H_A: \text{ At least one of } \beta_1, \beta_2, \beta_3 \neq 0
$$

We found that there was a linear association between at least one of the coefficient-covariate pairs and the response variable using our F-test. 

\end{bluebox}

## (iii) 

What is an estimated value of $\sigma^2$ based on the results shown in the above ANOVA table?

\begin{bluebox}
The value of $\sigma^2$ can be estimated by $\hat{\sigma^2} = MSE = 0.15306$. 
\end{bluebox}

## (iv) 

Under the null hypothesis $H_0$ specified in Part (ii), find the distribution of $R^2$ and then compute $P(R^2 \geq 0.637538|H_0)$. What does the value of $P(R^2 \geq 0.637538|H_0)$ imply?

```{r}
prob4_4_quantity <- pbeta(0.637538, (3/2), (117/2), lower.tail = FALSE)
print(prob4_4_quantity)
```

\begin{bluebox}

Under the null hypothesis, our model would reduce to $Y = \beta_0 + \epsilon_i$. In multiple linear regression, under the null hypothesis the coefficient of determination is Beta-distributed with parameters that are equal to half of the degrees of freedom associated with the model and error. So, under the null hypothesis for this problem, $R^2 \sim \text{Beta} \left(\frac{3}{2}, \frac{117}{2}\right) $.

Now since we have a defined distribution for $R^2$ under the null hypothesis, we can compute the probability that $R^2$ exceeds $0.637538$ under the null hypothesis. We can generate this particular value using R, which turns out to be incredibly small. This extremely low probability indicates that it is incredibly improbable to observe a value of $R^2 \ge 0.637538$ given that the true model is $Y = \beta_0 + \epsilon_i$.

\end{bluebox}

\newpage

# Problem 5 

Consider the multiple regression model:
  $$
  Y_i=\beta_1 X_{i1}+\beta_{2} X_{i2} + 
  \epsilon_i, \;\; i=1,2,\dots,n,
  $$
 where the $\epsilon_i$ are uncorrelated, with $E[\epsilon_i]=0$ and
 Var$(\epsilon_i)=\sigma^2$, and $X_{1i}$ and $X_{2i}$ are two covariates.
 Let 
 
 $$
 X=\begin{pmatrix}
   X_{11} & X_{21} & \dots & X_{n1} \\
   X_{12} & X_{22} & \dots & X_{n2} 
 \end{pmatrix}',
 $$
 which is an $n \times 2$ matrix.
 Assume that $(X_{1j},X_{2j},\dots,X_{nj})' \ne (1,1,\dots,1)'$ for
 $j=1,2$  and $X'X$ is of full rank, i.e., $|X'X| \ne 0$.


## (a) 

Derive the normal equations using the LS criterion.

\begin{bluebox}

Using the LS criterion means minimizing the score function

$$
S(\bm{\beta}) = \sum^{n}_{i=1}(Y_i - \beta_1X_{i1}- \beta_2X_{i2})^2 = (\mathbf{Y - X}\bm{\beta})'(\mathbf{Y - X}\bm{\beta})
$$
To get the normal equations, we can differentiate this function w.r.t. $\bm{\beta}$ and subsequently set the derivative equal to $0$:

$$
\frac{\partial S}{\partial \bm{\beta}} = -2 \bm{X}' \bm{Y} + 2\bm{X}'\bm{X}\bm{\beta} \implies \bm{X}'\bm{X}\bm{\beta} = \bm{X}'\bm{Y}
$$
\end{bluebox}

\newpage
## (b) 

Derive the LS estimators of $\beta_1$ and $\beta_2$. 

\begin{bluebox}
The LS estimators of $\beta_1$ and $\beta_2$ are the selections of the elements of $\bm{\beta}$ that solve the normal equations and are $\hat{\bm{\beta}} = (\bm{X}'\bm{X})^{-1} \bm{X}'\bm{Y}$. 

The Gram matrix $\bm{X}'\bm{X}$ is a matrix in $\mathbb{R}^{2\times2}$ which we can calculate to be:

$$
\begin{pmatrix}
   X_{11} & X_{21} & \dots & X_{n1} \\
   X_{12} & X_{22} & \dots & X_{n2} 
 \end{pmatrix} 
 \begin{pmatrix}
   X_{11} & X_{12} \\
   X_{21} & X_{22} \\
   \vdots & \vdots \\
   X_{n1} & X_{n2}
 \end{pmatrix}
 = 
\begin{pmatrix}
\sum^n_{i=1} X_{i1}^2 & \sum^n_{i=1} X_{i1}X_{i2}\\
\sum^n_{i=1}X_{i1}X_{i2} & \sum^n_{i=1} X_{i2}^2
\end{pmatrix}
$$
The inverse of this matrix can be expressed by first finding the determinant, $|\bm{X}'\bm{X}| = (\sum^n_{i=1} X_{i1}^2)(\sum^n_{i=1} X_{i2}^2) - \left(\sum^n_{i=1} X_{i1}X_{i2} \right)^2$, and then expressing the matrix as:

$$
\frac{1}{(\sum^n_{i=1} X_{i1}^2)(\sum^n_{i=1} X_{i2}^2) - \left(\sum^n_{i=1} X_{i1}X_{i2} \right)^2} 
\begin{pmatrix}
\sum^n_{i=1} X_{i2}^2 & -\sum^n_{i=1} X_{i1}X_{i2}\\
-\sum^n_{i=1}X_{i1}X_{i2} & \sum^n_{i=1} X_{i1}^2
\end{pmatrix}
$$

This inverse Gram matrix is then multiplied by:

$$
\bm{X}'\bm{Y} = 
\begin{pmatrix}
   X_{11} & X_{21} & \dots & X_{n1} \\
   X_{12} & X_{22} & \dots & X_{n2} 
 \end{pmatrix} 
\begin{pmatrix}
Y_1 \\
Y_2 \\
\vdots\\ 
Y_n
\end{pmatrix} =
\begin{pmatrix}
\sum^n_{i=1} Y_iX_{i1} \\
\sum^n_{i=1} Y_iX_{i2}
\end{pmatrix}
$$

So, together, our expression for the LS estimators of $\beta_1$ and $\beta_2$ is:

$$
\hat{\bm{\beta}} = \frac{1}{(\sum^n_{i=1} X_{i1}^2)(\sum^n_{i=1} X_{i2}^2) - \left(\sum^n_{i=1} X_{i1}X_{i2} \right)^2} 
\begin{pmatrix}
\sum^n_{i=1} X_{i2}^2 & -\sum^n_{i=1} X_{i1}X_{i2}\\
-\sum^n_{i=1}X_{i1}X_{i2} & \sum^n_{i=1} X_{i1}^2
\end{pmatrix}
\begin{pmatrix}
\sum^n_{i=1} Y_iX_{i1} \\
\sum^n_{i=1} Y_iX_{i2}
\end{pmatrix} = 
$$

$$
= \frac{1}{(\sum^n_{i=1} X_{i1}^2)(\sum^n_{i=1} X_{i2}^2) - \left(\sum^n_{i=1} X_{i1}X_{i2} \right)^2}
\begin{pmatrix}
  \sum^n_{i=1} X_{i2}^2 \sum^n_{i=1} Y_iX_{i1} - \sum^n_{i=1}X_{i1}X_{i2}\sum^n_{i=1} Y_iX_{i2} \\
  -\sum^n_{i=1}X_{i1}X_{i2}\sum^n_{i=1} Y_iX_{i1} + \sum^n_{i=1} X_{i1}^2\sum^n_{i=1} Y_iX_{i2}
\end{pmatrix}
$$

$$
\therefore \hat{\bm{\beta}} = \begin{pmatrix}
  \frac{\sum^n_{i=1} X_{i2}^2 \sum^n_{i=1} Y_iX_{i1} - \sum^n_{i=1}X_{i1}X_{i2}\sum^n_{i=1} Y_iX_{i2}}{(\sum^n_{i=1} X_{i1}^2)(\sum^n_{i=1} X_{i2}^2) - \left(\sum^n_{i=1} X_{i1}X_{i2} \right)^2} \\
  \frac{-\sum^n_{i=1}X_{i1}X_{i2}\sum^n_{i=1} Y_iX_{i1} + \sum^n_{i=1} X_{i1}^2\sum^n_{i=1} Y_iX_{i2}}{(\sum^n_{i=1} X_{i1}^2)(\sum^n_{i=1} X_{i2}^2) - \left(\sum^n_{i=1} X_{i1}X_{i2} \right)^2}
\end{pmatrix}
$$

\end{bluebox}

\newpage
## (c)  

Let $e_i$ denote the residual and also let $\hat{Y}_i$ denote the fitted value for $i=1,2,\dots, n$.  Which of the following statements are true?

### (c1) $\sum^n_{i=1} e_i=0$ always.
   
\begin{bluebox}
\textbf{FALSE}: This is true in SLR and MLR cases when an intercept is included, because a normal equation would give $\sum e_i = 0$, but for our particular intercept-free model, this constraint is not forced by our normal equations.
\end{bluebox}

### (c2) $\sum^n_{i=1} e_i \hat{Y}_i=0$ always.

\begin{bluebox}
\textbf{TRUE}: Under the least squares method, residuals are always orthogonal to fitted values. Least squares gives us $\hat{\bm{Y}} = \bm{X}\hat{\bm{\beta}}$ and so $\bm{e}'\hat{\bm{Y}} = \bm{e}'\bm{X}\hat{\bm{\beta}} = (\bm{X}'\bm{e})'\hat{\bm{\beta}} = 0 $ because $(\bm{X}'\bm{e} = 0) due to the normal equations dictating orthogonality of the predictors and residuals. 
\end{bluebox}

### (c3) $\sum^n_{i=1} e_i X_{ij}=0$ always for $j=1,2$.

\begin{bluebox}
\textbf{TRUE}: This is true, and is how we proved the prior statement was true. For SLR and MLR regression models using predictors $X_{.j}$, each predictor will generate a normal equation where $\sum^n_{i=1} e_i X_{ij}=0$.
\end{bluebox}

### (c4) $\sum^n_{i=1} e_i {Y}_i=0$ always.
 
\begin{bluebox}
\textbf{FALSE}: We know that we can represent the true $\bm{Y}$ values as $\bm{Y} = \hat{\bm{Y}} + \bm{e}$, and pre-multiplying by $\bm{e}$ we get $\bm{e}'\bm{Y} = \bm{e}'\hat{\bm{Y}} + \bm{e}'\bm{e}$. In part c2, we showed that $\bm{e}'\hat{\bm{Y}} = 0$. Following that realization we get $\bm{e}'\bm{Y} = \sum e^2_i$. This is only zero if all residuals are 0, which does not happen always (or nearly ever in practice). Therefore, the statement is false.
\end{bluebox}

\newpage 

# Problem 6

In a data set with 2 covariates and 100 observations, the sample variance for the responses is 8.158. When fitting a linear regression on the data, the $F$ statistics for testing the usefulness of the overall model is 41.26, the residual for the first observation is $-2.393$, and its standard error is $\sqrt{4.343}$. Find the standard error of the fitted value for the first observation.

\begin{bluebox}
a
\end{bluebox}