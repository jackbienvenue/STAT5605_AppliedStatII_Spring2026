---
title: "STAT 5605 Homework 3"
subtitle: "February 19, 2026"
author: "Jack Bienvenue"
output:
  bookdown::pdf_document2:
    includes:
      in_header: preamble.tex
    latex_engine: pdflatex
    keep_tex: true 
header-includes:
  - \usepackage{titling}
  - \setlength{\droptitle}{1em} 
fontsize: 12pt
---

```{r, echo=FALSE}
invisible(lapply(c("ggplot2", "dplyr", "tidyr"), 
  function(pkg) suppressPackageStartupMessages(library(pkg, 
                                                character.only = TRUE))))
```

# Problem 1

A large, national grocery retailer tracks productivity and costs of its facilities closely. A subset of the data, HW3PR1.txt, was obtained from a single distribution center for a one-year period. The variables included are $X_1=$ the number of cases shipped, $X_2=$ the indirect costs of the total labor hours as a percentage, and $X_3=1$ if the week has a holiday and 0 otherwise. The response variable $Y$ is the total labor hours.

```{r}
HW3PR1 <- read.table("./HW3PR1.txt", header=TRUE)
str(HW3PR1)
nrow(HW3PR1)
PR1 <- lm(formula = Y ~ X1+X2+X3, data=HW3PR1)
summary(PR1)
new_x_value <- data.frame(X1 = 400000, X2=7.2, X3=0)
predicted_value1 <- predict(object = PR1, newdata = new_x_value, 
                            interval="confidence")
print(predicted_value1)
predicted_value2 <- predict(object = PR1, newdata = new_x_value, 
                            interval="prediction")
print(predicted_value2)
```

## (a) 

Create a multivariate scatter plot matrix for $X_1$,$X_2$, $X_3$, and $Y$. What information does this plot provide?

```{r}
HW3PR1a <- select(HW3PR1, X1,X2,X3,Y)
pairs(HW3PR1a)
```

\begin{bluebox}
The scatter plot matrix given by this problem allows us to see scatter plots for each possible pair combination of covariates and the response variable. These scatterplots are useful to gauge whether we may have a linear association between variables. This helps in three ways: first, we can observe whether or not it seems apparent that there is a linear association between a covariate and response (which helps us determine whether regression on a variable may explain some variation in $Y$), second we can observe whether we may have a reason to believe that there is collinearity in our covariates, and third, we can assess the direction of the linear association if there appears to be an association. Further, the scatterplot matrix can help us view the structure of the data for each variable, as we may be interested in the range of the data or whether the data has factor levels. 

This particular scatterplot matrix has some interesting information. A clear linear association does not appear to exist between $X_1$ and $X_2$. The coincidence of the binary variable $X_3$ does not show clear clustering of $X_3$ in relation to values of $X_1$ and $X_2$. 

For relationships with $Y$, we do not see very clear signs of linear association for $X_1$ and $X_2$ with $Y$. We do see a clear grouping of the holidays in $X_3$ related to $Y$: it appears that all the holidays have higher associated values of $Y$. We can determine whether or not linear associations exist for $X_1$ and $X_2$ and $Y$ and whether using a dummy variable for $X_3$ helps with explaining variation in $Y$ more formally using hypothesis testing techniques. This graphical method is good for exploring data initially but we may not obtain the full story of the variables' relationships from this scatter plot matrix. 
\end{bluebox}

## (b)

The cases in HW3PR1.txt are given in consecutive weeks.  Prepare a time plot for each predictor variable as well as the response variable $Y$. What do these plots show?

```{r}
# Data prep for nice time series plot:
HW3PR1_long <- HW3PR1 %>% mutate(Week = case) %>%
  select(Week, Y, X1, X2, X3) %>%
  pivot_longer( cols = c(Y, X1, X2, X3),
                 names_to = "Variable",
                   values_to = "Value") 
HW3PR1_long$Variable <- recode(HW3PR1_long$Variable,
  Y  = "Total Labor Hours (Y)",
  X1 = "Cases Shipped (X1)",
  X2 = "Indirect Labor % (X2)",
  X3 = "Holiday Indicator (X3)"
)

# 2x2 Scatterplot of the time series for the variables:
ggplot(HW3PR1_long, aes(x = Week, y = Value)) +
  geom_point(alpha = 0.6, color = "gray40") +
  geom_smooth(
    method = "gam",
    formula = y ~ s(x, bs = "cs"),  # cubic spline fit 
    se = TRUE,
    color = "#376199",
    linewidth = 1
  ) +
  
  facet_wrap(~ Variable, ncol = 2, scales = "free_y") +
  
  labs(
    title = "Weekly Time Series for Grocery Store Data Variables",
    x = "Week",
    y = "Value",
    caption = "Cubic Spline Fit Used for Time Series | Jack Bienvenue 2026"
  ) +
  
  theme_minimal(base_size = 13) +
  theme(
  plot.title = element_text(
    face = "bold",
    hjust = 0.5),
  axis.title = element_text(
    face = "bold"),
  plot.caption = element_text(
    color = "gray60"))
```

\begin{bluebox}
In this part of the problem, we would like to assess whether there is clear seasonality in the trends of any of the predictors or the response variable. We should realize that, if we were to create predictive models or make inferences based upon this data, that inferences or predictions may be inaccurate because of time-dependence issues. 

These plots provide several insights into how time dependence plays a role in this data. For the top-left plot, we observe a potentially meaningful time series signal of cases shipped increasing on average towards the end of the year, potentially associated with the holiday season assuming that Week 0 indicates the beginning of the calendar year. Following this assumption, we see shipments gradually decrease on average as the winter and spring progress, bottoming in the summer and increasing again on average as the fall progresses. 

The signal for indirect labor percentages is less clear, but suggests that the percentage may gradually increase on average as the year progresses.

The holiday indicator's plot is not helpful in its own right, but provides clarity on where the holidays occur during the year. We can observe that $12\%$ of the weeks in the year are considered holidays.

When reviewing the outcome variable, we can see that the total labor hours are on average relatively stagnant throughout the year, not exhibiting obvious seasonality. However, in combination with the holiday indicator plot, we can observe that in this data set, the highest total labor hours week coincide with the holiday weeks. 

We should note when reviewing these time series plots that we are observing the time series over a single year and that the trends we observe over a single year may not be fully representative of the true time series for these variables. 
\end{bluebox}


## (c) 

Test whether there is a regression relation. State the hypotheses and conclusion. What does your test result imply about regression coefficients (slopes) associated with $X_1$, $X_2$, and $X_3$?  What is the p-value of the test?

\begin{bluebox}
Before performing our test, let's set the hypotheses for our testing:

$$
H_0: \beta_1 = \beta_2 = \beta_2 = 0, \text{ versus}
$$

$$
H_A: \text{At least one of } \beta_1, \beta_2, \beta_3 \neq 0.
$$

We select this particular pair of hypotheses because we testing simply for whether a regression relation exists in our linear model, regardless of whether every single regressor exhibits a regression relation with the outcome variable. Furthermore, we will decide to use an $\alpha = 0.05$ significance level for our test.

Our regression output provides information for this test automatically, with a p-value of $4.907\times 10^{-12}$, far below the designated threshold. Therefore, we will reject the null hypothesis which stated that none of the regression coefficients ${\beta_1, \beta_2, \beta_3}$ were nonzero. We have reason to believe that the model overall has a linear regression relationship with the outcome variable. In essence, an MLR model based upon the number of cases shipped, the indirect costs of total labor hours (\%) and holiday indicator helps to explain some of the variation in total labor hours at the supermarket.
\end{bluebox}

## (d) 

Calculate the coefficient multiple determination $R^2$. How is this measure interpreted here.

\begin{bluebox}
The multiple $R^2$ is included in our regression output and is equal to $0.6985$. This means that the linear relationship between $Y$ and $X_1, X_2$, and $X_3$ explains about $69.85\%$ of the variability in $Y$, the total labor hours in a week at the grocery store.
\end{bluebox}

## (e) 

For the data, HW3PR1.txt, on which the regression fit is based, would you consider a shipment of 400,000 cases with an indirect percentage of 7.2 on a non-holiday week to be within the scope of the model? What about a shipment of 400,000 cases with an indirect percentage of 9.9 on a non-holiday week? Support your answers by preparing or referring to a relevant plot.

```{r}
# Check sample range of data

## X_1
summary(HW3PR1$X1)

## X_2
summary(HW3PR1$X2)

## Y
summary(HW3PR1$Y)
```


``` {r, echo=FALSE, fig.width = 8, fig.height = 8}
library(ggplot2)
library(dplyr)
library(tidyr)

# Build ordered range dataframe
range_df <- HW3PR1 %>%
  summarise(
    X1_min = min(X1), X1_max = max(X1),
    X2_min = min(X2), X2_max = max(X2),
    X3_min = min(X3), X3_max = max(X3)
  ) %>%
  pivot_longer(everything(),
               names_to = c("variable", ".value"),
               names_pattern = "(X[123])_(min|max)") %>%
  mutate(
    variable = factor(variable,
                      levels = c("X1", "X2", "X3"),
                      labels = c("Cases Shipped (X1)",
                                 "Indirect Costs Ttl. Lbr., % (X2)",
                                 "Holiday Indicator (X3)"))
  )

# Proposed observations
new_points <- data.frame(
  variable = factor(rep(c("Cases Shipped (X1)",
                          "Indirect Costs Ttl. Lbr., % (X2)",
                          "Holiday Indicator (X3)"), 2),
                    levels = levels(range_df$variable)),
  value = c(400000, 7.2, 0,
            400000, 9.9, 0),
  obs = rep(c("(400000, 7.2, 0)",
              "(400000, 9.9, 0)"), each = 3)
)

ggplot() +
  
  # Thin sample range lines (all predictors)
  geom_segment(data = range_df,
               aes(x = min, xend = max,
                   y = 0, yend = 0),
               linewidth = 1,
               color = "grey30",
               alpha = 0.7) +
  
  # Proposed observation points
  geom_point(data = new_points,
             aes(x = value, y = 0, color = obs),
             size = 4,
             alpha = 0.7) +
  
  facet_wrap(~ variable,
             ncol = 1,
             scales = "free_x") +
  
  labs(title = "New Observation X Values vs Sample Ranges",
       x = "Value",
       y = NULL,
       color = NULL) +
  
  theme_minimal(base_size = 14) +
  theme(
    strip.text = element_text(size = 14, face = "bold"),
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank(),
    legend.position = "bottom",
    panel.spacing = unit(1.5, "lines")
  )
```

\begin{bluebox}
Let's review the sample range of the variables from which we built our regression model.

\textbf{SAMPLE RANGES:}

\begin{enumerate}
  \item $X_1$: $[211944, 472476]$ (\# Cases Shipped),
  \item $X_2$: $[4.610, 9.650]$ (Indirect Labor $\%$),
  \item $X_3$: $\\{0,1\\}$ (Holiday Indicator $1=\text{Holiday}$),
  \item $Y$: $[3998, 5045]$ (Total Labor Hours)
\end{enumerate}

So, for an observation $\begin{pmatrix} 400000 \\ 7.2 \\ 0 \\ Y_j\end{pmatrix}$, every $X_i, i=1,2,3$ element is within the range of data used to generate our least squares fit. Therefore, we can consider this observation to be within the scope of the model.

For an observation $\begin{pmatrix} 400000 \\ 9.9 \\ 0 \\ Y_{j'}\end{pmatrix}$, the value of $X_2$ exceeds the range of the data used to construct the model, and therefore the observation is outside the scope of the model.

In the figure above, we can see where the two observations' feature values lay in relation to the range of data used to construct the model. 

\end{bluebox}

## (f)  

Assume that multiple regression model for three predictor variables with independent normal error terms is appropriate. A new shipment is to be received with $x_{h1}=292,087$, $x_{h2}=7.77$, and $x_{h3}=0$.

### (f1) 

Obtain a 95\% confidence interval for the expected total labor hours for this shipment.

```{r}
# Find relevant t critical value for computing intervals:
t_0.975_46 <- qt(0.975, 46)
cat("Critical t:", t_0.975_46)
print('')

# Verify fitted value using model: 
new_obs <- data.frame(X1 = 292087, X2 = 7.77, X3 = 0)
predict(PR1, new_obs, interval = "confidence", level = 0.95)
```
  
\begin{bluebox}

We can start by building a $95\%$ confidence interval for the expected total labor hours for this shipment using the construction:

$$
95\% \text{ Conf. Int.} = \hat{Y_h} \pm t_{0.975, n-4}\times \text{SE}(\hat{Y_h}) = \hat{Y_h} \pm t_{0.975, n-4}\times  \sqrt{\hat{\sigma^2}\bm{x}_h' (\bm{X}'\bm{X})^{-1}\bm{x_h}},
$$

which is equal to $ (4230.776 , 4328.539)$.

To calculate our confidence interval bounds, we must obtain the fitted value $\hat{Y_h} = \bm{x}_h'\hat{\bm{\beta}}$. Using our fitted models and known value for the observed $X_{ih}, i-1,2,3$, we can calculate the fitted value as $\begin{pmatrix}1 & 292087 & 7.77 & 0 \end{pmatrix} \begin{pmatrix} 4100 \\ 0.0009343 \\ -12.00\\ 613.9\end{pmatrix} = 279.658 $

The critical value for creating a $95\%$ confidence interval in this scenario is $t_{0.975, n-4} = t_{0.975, 46} = 2.012896$. The standard error for the fitted mean is $\text{SE}(\hat{Y_h}) = \sqrt{\hat{\sigma^2}\bm{x}_h' (\bm{X}'\bm{X})^{-1}\bm{x_h}}$.

\end{bluebox}

### (f2) 

Compute a 95\% prediction interval for the total labor hours for this new shipment.

```{r}
# Use model to predict interval:
predict(PR1, new_obs, interval = "prediction", level = 0.95)
```

\begin{bluebox}
We can continue by building a $95\%$ prediction interval for the total labors hours for this new shipment using the construction:

Here, what changes for a prediction interval as opposed to the confidence interval for the expected total hours is an expansion of the interval by inflating the standard error. For the prediction interval, our standard error formulation is changed to $\text{SE}_{\text{pred}}(\hat{Y_h}) = \sqrt{\hat{\sigma^2}(1 + \bm{x}_h' (\bm{X}'\bm{X})^{-1}\bm{x_h})}$. So, our interval is:

$$
95\% \text{ Pred. Int.} = \hat{Y}_h \pm t_{0.95, n-4} \times \sqrt{\hat{\sigma^2}(1 + \bm{x}_h' (\bm{X}'\bm{X})^{-1}\bm{x_h})} = (3987.24 , 4572.075).
$$

\end{bluebox}

\newpage

# Problem 2

An analyst wanted to fit a regression model $Y_i=\beta_0+\beta_1X_{i1} + \beta_{2} X_{i2} + \beta_3 X_{i3} + \epsilon_i$, $i=1,2,\dots,n$, by the method of least squares when it is known that $\beta_2=5$. How can the analyst obtain the desired fit by using a multiple regression computer program? What if it is known that $\beta_3=0$?

\begin{bluebox}
\textbf{First Case:}
To generate a least squares fit for MLR where one coefficient (e.g., $\beta_2 = 0$) is already known, we can adjust our response variable by subtracting the true coefficient multiplied by its input covariate $X_{i2}$, changing our output variable for our MLR to be $Y^{\star}_i = Y_i - 5X_{i2}$. Then, we can use the usual least squares method to regress the adjusted response on the remaining covariates and unknown parameters,

$$
Y^\star_i = \beta_0 + \beta_1 X_{i1} + \beta_3 X_{i3} + \epsilon_i
$$


\textbf{Second Case:}
I'm going to assume that the problem statement indicates that $\beta_2$ is now unknown again and that $\beta_3$ alone is known, and is equal to zero. In this circumstance, the model can be reduced to $Y_i=\beta_0+\beta_1X_{i1} + \beta_{2} X_{i2} + \epsilon_i$ and fit using least squares as a MLR model regressing on $X_{i1}$ and $X_{i2}$ with an intercept term included. If we do actually know that $\beta_2 = 5$, then our regression model could become:

$$
Y^\star_i = \beta_0 + \beta_1 X_{i1} + \epsilon_i.
$$

So, if the analyst is clever about how they implement the known values, they can proceed with building regression models with no issue. 
\end{bluebox}

\newpage

# Problem 3 

Consider the multiple linear regression model:   $Y_i=\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i1} X_{i2} + \epsilon_i$, where $\epsilon_i \sim N(0,\sigma^2)$, $i=1,2,\dots,n$. Which of the following statements would be correct?  Provide a necessary explanation.

## (a)  

The number of predictors is 2.

\begin{bluebox}
\textbf{INCORRECT*} - We have three predictors that we are using for our model: $X_{i1}$,$X_{i2}$, and $X_{i1}X_{i2}$.

However, if we are considering predictors to be unique features, we have two unique features here: $X_1$ and $X_2$. We also have an interaction term $X_1X_2$ formed from $X_1$ and $X_2$. Under this circumstance, we could claim that this statement is true.
\end{bluebox}

## (b)  

The number of regressors is 3.
  
\begin{bluebox}
\textbf{INCORRECT*} - We are using four regressors here, $1, X_{i1}, X_{i2}, \text{ and } X_{i1}X_{i2}$, multiplied by $\beta_0, \beta_1, \beta_2, \beta_3$, respectively. 

If we do not consider the intercept term to be a regressor, then we could say we have three distinct regressors for $Y$: $\beta_1X_{i1}$, $\beta_2 X_{i2}$, $\beta_3 X_{i1}X_{i2}$.
\end{bluebox}

## (c)  

The number of model parameters is 3.

\begin{bluebox}
\textbf{INCORRECT} - We have five distinct parameters for our model: $\beta_0, \beta_1, \beta_2, \beta_3, \sigma^2$.
\end{bluebox}

## (d)  

The number of model parameters is 4.

\begin{bluebox}
\textbf{INCORRECT} - We have five distinct parameters for our model: $\beta_0, \beta_1, \beta_2, \beta_3, \sigma^2$.
\end{bluebox}

## (e)  

The number of model parameters is 5.

\begin{bluebox}
\textbf{CORRECT} - the model parameters are $\beta_0, \beta_1, \beta_2, \beta_3$ and the variance of the error term, $\sigma^2$. This makes for a total of $5$ parameters in the model. 
\end{bluebox}

## (f)  

The number of model parameters is $\infty$.

\begin{bluebox}
\textbf{INCORRECT} - our model has closed form $Y_i=\beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i1} X_{i2} + \epsilon_i$, where $\epsilon_i \sim N(0,\sigma^2)$, $i=1,2,\dots,n$ with $5$ uniquely identifiable parameters as we observed for the previous statement. The number of model parameters therefore is finite and equal to $5$, the number of model parameters is not $\infty$.
\end{bluebox}

## (g)  

None of the above.

\begin{bluebox}
\textbf{INCORRECT} - We found at least \textbf{part e} to be true.
\end{bluebox}


# Problem 4 

An analysis is performed to study the relationship between three explanatory variables, $X_1$, $X_2$ and $X_3$, and a response variable $Y$.  Consider the following model:
$$
Y=\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3+ \epsilon
$$
where $\epsilon \sim N(0,\sigma^2)$. The observed data is denoted by
 $\{(Y_i,X_{1i},X_{2i},X_{3i}),\; i=1,2,\dots,n\}$ and we further assume that
the $Y_i$'s are independent. We fit the above model to this data set.
The resulting ANOVA table is given below and  the coefficient of determination is 0.637538.

\begin{center}
{\bf The  ANOVA Table}

\begin{tabular}{lcrrrr} \hline
\multicolumn{6}{c}{Analysis of Variance} \\ \hline
Source  & DF       & Sum of Square  & Mean Square &  F Stat & Prob $>$ F \\ \hline
Model   & $\ast$   &   $\ast$       &   $\ast$    &  $\ast$ &   $\ast$ \\
Error   &  117      &  17.90761    &  0.15306    &         &   \\
C Total & $\ast$   &   $\ast$     &              &         & \\ \hline
\end{tabular}
\end{center}

Answer the following questions:

## (i) 

Fill in the missing values (denoted by ``$\ast$'') in the ANOVA table.

```{r}
prob4_1_quantity <- pf(65.72, 3, 117, lower.tail = FALSE)
print(prob4_1_quantity)
```

\begin{bluebox}
In this problem, we are given a small amount of information, but we should be able to work backwards to get all of the quantities we need to fill our table.

Besides the information about the error degrees of freedom, associated SS, and MS, we know that the coefficient of determination $R^2 = 0.627538 = \frac{SSR}{SSTO} = 1 - \frac{SSE}{SSTO}.$ Right away, we can actually find $SSTO$ and $SSR$:

$$
\frac{SSE}{SSTO} = 1 - 0.627538,
$$
Leading to:

$$
\frac{17.90761}{SSTO} = 0.372462 \implies SSTO = \frac{17.90761}{0.372462} \approx 48.07903.
$$

Therefore our $SSR$ is:

$$
SSR = 0.627538 \times 48.07903 \approx 30.17142.
$$

Moving on, we know in MLR that the degrees of freedom for the error term is $n-p$, with $p$ representing the number of parameters to be estimated. Here, we have four parameters to be estimated, so using our given error degrees of freedom $117$, we can find that our sample size is $n=121$. To find the model and total degrees of freedom, we recognize that the MLR model degrees of freedom is equal to the number of predictors, $3$, and the corrected total degrees of freedom is equal to $n-1$, so here the corrected total degrees of freedom would be $120$.

To find the relevant mean square error for the regression model, we simply need to divide the SSR by its corresponding degrees of freedom, $\frac{30.17142}{3} = 10.05714.$

Finally, we can round out our table by finding the F Statistic and its corresponding p-value by first identifying the F Statistic's value, $F = \frac{MSR}{MSE} = \frac{10.05714}{0.15306} \approx 65.70717 $. This means that we can find the p-value through calculating the probability $P(F_{3,117} > 65.72)$. This probability will be extremely close to zero. Comfortably we can claim that the p-value would end up being $\ll 0.0001$.

So, our completed table is:

\begin{center}
{\bf The  ANOVA Table}

\begin{tabular}{lcrrrr} \hline
\multicolumn{6}{c}{Analysis of Variance} \\ \hline
Source  & DF       & Sum of Square  & Mean Square &  F Stat & Prob $>$ F \\ \hline
Model   & 3   &   30.17142       &   10.05714    &  65.72 &   $\ll 0.0001$\\
Error   &  117      &  17.90761    &  0.15306    &         &   \\
C Total & 120   &   48.07903     &              &         & \\ \hline
\end{tabular}
\end{center}
\end{bluebox}

## (ii) 

State the null and alternative hypotheses ($H_0$ and $H_1$) for the $F$ test in the ANOVA table.

\begin{bluebox}
For the type of F test employed by our table setup, we are testing whether the linear regression model overall explains any variation in Y. Therefore we are testing the hypothesis:

$$
H_0: \beta_1 = \beta_2 = \beta_3 = 0, \text{ against}
$$

$$
H_A: \text{ At least one of } \beta_1, \beta_2, \beta_3 \neq 0
$$

We found that there was a linear association between at least one of the coefficient-covariate pairs and the response variable using our F-test. 

\end{bluebox}

## (iii) 

What is an estimated value of $\sigma^2$ based on the results shown in the above ANOVA table?

\begin{bluebox}
The value of $\sigma^2$ can be estimated by $\hat{\sigma^2} = MSE = 0.15306$. 
\end{bluebox}

## (iv) 

Under the null hypothesis $H_0$ specified in Part (ii), find the distribution of $R^2$ and then compute $P(R^2 \geq 0.637538|H_0)$. What does the value of $P(R^2 \geq 0.637538|H_0)$ imply?

```{r}
prob4_4_quantity <- pbeta(0.637538, (3/2), (117/2), lower.tail = FALSE)
print(prob4_4_quantity)
```

\begin{bluebox}

Under the null hypothesis, our model would reduce to $Y = \beta_0 + \epsilon_i$. In multiple linear regression, under the null hypothesis the coefficient of determination is Beta-distributed with parameters that are equal to half of the degrees of freedom associated with the model and error. So, under the null hypothesis for this problem, $R^2 \sim \text{Beta} \left(\frac{3}{2}, \frac{117}{2}\right) $.

Now since we have a defined distribution for $R^2$ under the null hypothesis, we can compute the probability that $R^2$ exceeds $0.637538$ under the null hypothesis. We can generate this particular value using R, which turns out to be incredibly small. This extremely low probability indicates that it is incredibly improbable to observe a value of $R^2 \ge 0.637538$ given that the true model is $Y = \beta_0 + \epsilon_i$.

\end{bluebox}

\newpage

# Problem 5 

Consider the multiple regression model:
  $$
  Y_i=\beta_1 X_{i1}+\beta_{2} X_{i2} + 
  \epsilon_i, \;\; i=1,2,\dots,n,
  $$
 where the $\epsilon_i$ are uncorrelated, with $E[\epsilon_i]=0$ and
 Var$(\epsilon_i)=\sigma^2$, and $X_{1i}$ and $X_{2i}$ are two covariates.
 Let 
 
 $$
 X=\begin{pmatrix}
   X_{11} & X_{21} & \dots & X_{n1} \\
   X_{12} & X_{22} & \dots & X_{n2} 
 \end{pmatrix}',
 $$
 which is an $n \times 2$ matrix.
 Assume that $(X_{1j},X_{2j},\dots,X_{nj})' \ne (1,1,\dots,1)'$ for
 $j=1,2$  and $X'X$ is of full rank, i.e., $|X'X| \ne 0$.


## (a) 

Derive the normal equations using the LS criterion.

\begin{bluebox}

Using the LS criterion means minimizing the score function

$$
S(\bm{\beta}) = \sum^{n}_{i=1}(Y_i - \beta_1X_{i1}- \beta_2X_{i2})^2 = (\mathbf{Y - X}\bm{\beta})'(\mathbf{Y - X}\bm{\beta})
$$
To get the normal equations, we can differentiate this function w.r.t. $\bm{\beta}$ and subsequently set the derivative equal to $0$:

$$
\frac{\partial S}{\partial \bm{\beta}} = -2 \bm{X}' \bm{Y} + 2\bm{X}'\bm{X}\bm{\beta} \implies \bm{X}'\bm{X}\bm{\beta} = \bm{X}'\bm{Y}
$$
\end{bluebox}

\newpage
## (b) 

Derive the LS estimators of $\beta_1$ and $\beta_2$. 

\begin{bluebox}
The LS estimators of $\beta_1$ and $\beta_2$ are the selections of the elements of $\bm{\beta}$ that solve the normal equations and are $\hat{\bm{\beta}} = (\bm{X}'\bm{X})^{-1} \bm{X}'\bm{Y}$. 

The Gram matrix $\bm{X}'\bm{X}$ is a matrix in $\mathbb{R}^{2\times2}$ which we can calculate to be:

$$
\begin{pmatrix}
   X_{11} & X_{21} & \dots & X_{n1} \\
   X_{12} & X_{22} & \dots & X_{n2} 
 \end{pmatrix} 
 \begin{pmatrix}
   X_{11} & X_{12} \\
   X_{21} & X_{22} \\
   \vdots & \vdots \\
   X_{n1} & X_{n2}
 \end{pmatrix}
 = 
\begin{pmatrix}
\sum^n_{i=1} X_{i1}^2 & \sum^n_{i=1} X_{i1}X_{i2}\\
\sum^n_{i=1}X_{i1}X_{i2} & \sum^n_{i=1} X_{i2}^2
\end{pmatrix}
$$
The inverse of this matrix can be expressed by first finding the determinant, $|\bm{X}'\bm{X}| = (\sum^n_{i=1} X_{i1}^2)(\sum^n_{i=1} X_{i2}^2) - \left(\sum^n_{i=1} X_{i1}X_{i2} \right)^2$, and then expressing the matrix as:

$$
\frac{1}{(\sum^n_{i=1} X_{i1}^2)(\sum^n_{i=1} X_{i2}^2) - \left(\sum^n_{i=1} X_{i1}X_{i2} \right)^2} 
\begin{pmatrix}
\sum^n_{i=1} X_{i2}^2 & -\sum^n_{i=1} X_{i1}X_{i2}\\
-\sum^n_{i=1}X_{i1}X_{i2} & \sum^n_{i=1} X_{i1}^2
\end{pmatrix}
$$

This inverse Gram matrix is then multiplied by:

$$
\bm{X}'\bm{Y} = 
\begin{pmatrix}
   X_{11} & X_{21} & \dots & X_{n1} \\
   X_{12} & X_{22} & \dots & X_{n2} 
 \end{pmatrix} 
\begin{pmatrix}
Y_1 \\
Y_2 \\
\vdots\\ 
Y_n
\end{pmatrix} =
\begin{pmatrix}
\sum^n_{i=1} Y_iX_{i1} \\
\sum^n_{i=1} Y_iX_{i2}
\end{pmatrix}
$$

So, together, our expression for the LS estimators of $\beta_1$ and $\beta_2$ is:

$$
\hat{\bm{\beta}} = \frac{1}{(\sum^n_{i=1} X_{i1}^2)(\sum^n_{i=1} X_{i2}^2) - \left(\sum^n_{i=1} X_{i1}X_{i2} \right)^2} 
\begin{pmatrix}
\sum^n_{i=1} X_{i2}^2 & -\sum^n_{i=1} X_{i1}X_{i2}\\
-\sum^n_{i=1}X_{i1}X_{i2} & \sum^n_{i=1} X_{i1}^2
\end{pmatrix}
\begin{pmatrix}
\sum^n_{i=1} Y_iX_{i1} \\
\sum^n_{i=1} Y_iX_{i2}
\end{pmatrix} = 
$$

$$
= \frac{1}{(\sum^n_{i=1} X_{i1}^2)(\sum^n_{i=1} X_{i2}^2) - \left(\sum^n_{i=1} X_{i1}X_{i2} \right)^2}
\begin{pmatrix}
  \sum^n_{i=1} X_{i2}^2 \sum^n_{i=1} Y_iX_{i1} - \sum^n_{i=1}X_{i1}X_{i2}\sum^n_{i=1} Y_iX_{i2} \\
  -\sum^n_{i=1}X_{i1}X_{i2}\sum^n_{i=1} Y_iX_{i1} + \sum^n_{i=1} X_{i1}^2\sum^n_{i=1} Y_iX_{i2}
\end{pmatrix}
$$

$$
\therefore \hat{\bm{\beta}} = \begin{pmatrix}
  \frac{\sum^n_{i=1} X_{i2}^2 \sum^n_{i=1} Y_iX_{i1} - \sum^n_{i=1}X_{i1}X_{i2}\sum^n_{i=1} Y_iX_{i2}}{(\sum^n_{i=1} X_{i1}^2)(\sum^n_{i=1} X_{i2}^2) - \left(\sum^n_{i=1} X_{i1}X_{i2} \right)^2} \\
  \frac{-\sum^n_{i=1}X_{i1}X_{i2}\sum^n_{i=1} Y_iX_{i1} + \sum^n_{i=1} X_{i1}^2\sum^n_{i=1} Y_iX_{i2}}{(\sum^n_{i=1} X_{i1}^2)(\sum^n_{i=1} X_{i2}^2) - \left(\sum^n_{i=1} X_{i1}X_{i2} \right)^2}
\end{pmatrix}
$$

\end{bluebox}

\newpage
## (c)  

Let $e_i$ denote the residual and also let $\hat{Y}_i$ denote the fitted value for $i=1,2,\dots, n$.  Which of the following statements are true?

### (c1) $\sum^n_{i=1} e_i=0$ always.
   
\begin{bluebox}
\textbf{FALSE}: This is true in SLR and MLR cases when an intercept is included, because a normal equation would give $\sum e_i = 0$, but for our particular intercept-free model, this constraint is not forced by our normal equations.
\end{bluebox}

### (c2) $\sum^n_{i=1} e_i \hat{Y}_i=0$ always.

\begin{bluebox}
\textbf{TRUE}: Under the least squares method, residuals are always orthogonal to fitted values. Least squares gives us $\hat{\bm{Y}} = \bm{X}\hat{\bm{\beta}}$ and so $\bm{e}'\hat{\bm{Y}} = \bm{e}'\bm{X}\hat{\bm{\beta}} = (\bm{X}'\bm{e})'\hat{\bm{\beta}} = 0 $ because $(\bm{X}'\bm{e} = 0)$ due to the normal equations dictating orthogonality of the predictors and residuals. 
\end{bluebox}

### (c3) $\sum^n_{i=1} e_i X_{ij}=0$ always for $j=1,2$.

\begin{bluebox}
\textbf{TRUE}: This is true, and is how we proved the prior statement was true. For SLR and MLR regression models using predictors $X_{.j}$, each predictor will generate a normal equation where $\sum^n_{i=1} e_i X_{ij}=0$.
\end{bluebox}

### (c4) $\sum^n_{i=1} e_i {Y}_i=0$ always.
 
\begin{bluebox}
\textbf{FALSE}: We know that we can represent the true $\bm{Y}$ values as $\bm{Y} = \hat{\bm{Y}} + \bm{e}$, and pre-multiplying by $\bm{e}$ we get $\bm{e}'\bm{Y} = \bm{e}'\hat{\bm{Y}} + \bm{e}'\bm{e}$. In part c2, we showed that $\bm{e}'\hat{\bm{Y}} = 0$. Following that realization we get $\bm{e}'\bm{Y} = \sum e^2_i$. This is only zero if all residuals are 0, which does not happen always (or nearly ever in practice). Therefore, the statement is false.
\end{bluebox}

\newpage 

# Problem 6

In a data set with 2 covariates and 100 observations, the sample variance for the responses is 8.158. When fitting a linear regression on the data, the $F$ statistics for testing the usefulness of the overall model is 41.26, the residual for the first observation is $-2.393$, and its standard error is $\sqrt{4.343}$. Find the standard error of the fitted value for the first observation.

\begin{bluebox}
To get the standard error of a fitted value, we can use a formula that related this standard error to the mean squared error and leverage of the observation: $\text{SE}(\hat{Y}_1) = \sqrt{\text{MSE}\times h_{11}}$.

Let's see if the information provided can help us get to the leverage of the first observation and the mean squared error.

Leverage appears in the formula for the variance of the error term: $\text{Var}(e_i) = \sigma^2(1-h_{ii})$. The variance of the error can be replaced by $\text{SE}(e_i)^2 = (\sqrt{4.343})^2 = 4.343$ and we can substitute unknown $\sigma^2$ with its appropriate estimator $\text{MSE}$. So, we can isolate leverage:

$$
4.343 = \text{MSE}(1-h_{ii}) \implies h_{ii} = 1 - \frac{4.343}{\text{MSE}}.
$$

Now, we need to find $\text{MSE}$ to proceed with our calculation of the standard error for the fitted value of the first observation. 
 
We are given the F-statistic, and we know that the F-statistic is calculated as $F = \frac{MSR}{MSE} = \frac{\frac{SSR}{2}}{\frac{SSE}{97}}$. (These degrees of freedom represent the number of predictors, and $n-p$ where $p$ is the number of coefficients in the model).

The sample variance of $Y$ is $s_Y^2 = 8.158$, which can be related to the total sum of squares $\text{SSTO}$ which can help us to figure out the decomposition values of $\text{SSE}$ and $\text{SSR}$. The total sum of squares, $\text{SSTO} = (n-1)s^2_Y = 99 \times 8.158 = 807.642$. While we do not how the total sum of squares is distributed to $\text{SSR}$ and $\text{SSE}$, we can try to find these values, starting indirectly.

The regression sum of squares can alternatively be expressed as $\text{SSR} = F\times k \times \text{MSE}$, and the error sum of squares can alternatively be expressed as $\text{SSE} = (n-p)\text{MSE}.$ So, we can express the total sum of squares in terms of its decomposition: 

$$
\text{SSTO} = Fk\text{MSE} + (n-p)\text{MSE} \implies 807.642 = (41.26)(2)\text{MSE} + (97)\text{MSE},
$$
Which can help us yield $\text{MSE}$ as:

$$
\text{MSE} = \frac{807.642}{(41.26)(2)+97} \approx 4.499
$$

Now we can finally use our attained $\text{MSE}$ in order to calculate the standard error of the fitted value for the first observation:

$$
\text{SE}(\hat{Y}_1) = \sqrt{\text{MSE} \times \left( 1 - \frac{4.343}{\text{MSE}}\right)} = \sqrt{4.499 \times \left( 1 - \frac{4.343}{4.499}\right)} \approx 0.395.
$$
\end{bluebox}