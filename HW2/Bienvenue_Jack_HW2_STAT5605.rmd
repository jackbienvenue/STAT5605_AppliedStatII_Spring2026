---
title: "STAT 5605 Homework 2"
subtitle: "February 5, 2026"
author: "Jack Bienvenue"
output:
  bookdown::pdf_document2:
    includes:
      in_header: preamble.tex
    latex_engine: pdflatex
    keep_tex: true 
header-includes:
  - \usepackage{titling}
  - \setlength{\droptitle}{1em} 
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)

invisible(lapply(c( "ggplot2"), 
  function(pkg) suppressPackageStartupMessages(library(pkg, 
                                                character.only = TRUE))))
```


# Problem 1 

The least squares regression line for a given set of data with a sample size of $n=20$ is $\hat{Y}=-42 + 0.9 X$ (i.e., $b_0=-42$ and $b_1=0.9$). The MSE of the fitted simple linear regression (SLR) model is 0.14, the standard error of $b_1$ (i.e., $se(b_1)$) is 0.016, and the upper limit of a 95\% predictive interval at $X_{new}=222$ is 165.2.  Further, we assume that the error term has a normal distribution. Answer the following questions.

```{r}
# Calculate critical value for t:
t_crit_1a <- qt(0.975, df=18)
cat("t critical value:", t_crit_1a)
```

## (a) 

What is the standard error of the fitted value of $Y$ at $X=222$ (i.e., $se(\hat{Y})$)?

\begin{bluebox}
We can calculate $\text{se}(\hat{Y})$ by using the formula $\text{se}(\hat{Y}_{X=222}) = \sqrt{\text{MSE}\left(\frac{1}{n}+\frac{(\bar{X}-222)^2}{S_{XX}}\right)}$. So, we must find values for $\bar{X}$, $S_{XX}$.

The value $S_{XX}$ is not immediately available from the given information, but it can be retrieved from what we know about $0.016 = \text{se}(b_1) = \sqrt{\frac{\text{MSE}}{S_{XX}}}$. Given $\text{MSE} = 0.14$, we can rearrange the equation to become $S_{XX} = \frac{\text{MSE}}{(\text{se}(b_1))^2} = \frac{0.14}{0.016^2} = 546.875$.

To find $\bar{X}$ to calculate the standard error we are seeking, we will have to work backwards from the upper value of the $95\%$ predictive interval we are given. The upper end of this predictive interval is defined by the formula $\text{Upper Bound} = \hat{Y}(222)+t_{0.975, n-2}\sqrt{\text{MSE}\left(1+\frac{1}{n}+\frac{(222-\bar{X})^2}{S_{XX}} \right)}$. The fitted value $\hat{Y}$ at $x=222$ is:

$$
\hat{Y}(222) = -42+0.9(222) = 157.8.
$$
To calculate the value of $\bar{X}$, we can take our fitted value and input our known values into the equation for the upper bound and rearrange:

$$
\bar{X} =  222-\sqrt{S_{XX}\left(\frac{\left( \frac{\text{Upper Bound}-\hat{Y}(222)}{t_{0.975, n-2}} \right)^2}{\text{MSE}} - 1 -\frac{1}{n} \right)}
=
$$
$$
= 222-\sqrt{546.875\left(\frac{\left( \frac{165.2-157.8}{2.100922} \right)^2}{0.14} - 1 -\frac{1}{20} \right)} = 3.167
$$

Now, we can calculate the standard error: 

$$
\text{se}(\hat{Y}_{X=222}) = \sqrt{\text{0.14}\left(\frac{1}{20}+\frac{(3.167-222)^2}{546.875}\right)} = 3.502
$$

\end{bluebox}

## (b) 

What are the predicted value of $Y_{new}$ and its standard error at $X_{new}=222$?

\begin{bluebox}
In the prior part, we found that $\hat{Y}(222) = -42 + 0.9(222) =157.8$. Further, $se(Y_{new}) = \sqrt{\text{MSE}\left( 1 + \frac{1}{n} + \frac{x_{new} - \bar{X}^2}{S_{XX}} \right)}$, which we found to be $\approx 3.5$.
\end{bluebox}

## (c) 

How much does use of this SLR model reduce the uncertainty in predicting the response variable $Y$?

\begin{bluebox}
We can approach this problem by finding $R^2$. Since we don't explicitly have information about $SSR$, $SSE$, or $SSTO$, we can approach the $R^2$ calculation using the $R^2 = \frac{F}{F+(n-2)}$ formulation. We can calculate our F-statistic as $F = \frac{MSR}{MSE} = \frac{\hat{\beta}^2_1 S_{XX}}{MSE} = \frac{0.81\times546.875}{0.14} = 3164.06$. So, $R^2 = \frac{3164.06}{3164.06 + 18}= 0.9943.$

So, using this SLR model reduces uncertainty in predicting $Y$ by about $99.43\%$.
\end{bluebox}

\newpage

# Problem 2

An analysis is performed to study the relationship between a response variable $Y$ and a single explanatory variable $X$. The model guiding the analysis is:

$$
  Y_i=\beta_0+\beta_1 X_{i}+ \epsilon_i, 
$$ 
where the $\epsilon_i$ are independently and identically distributed as $N(0,\sigma^2)$ and $\sigma^2$ is unknown. The data with a sample size of $n=5$ were analyzed using PROC REG in SAS (or function lm in R) and Table 1 below shows the partial output produced by the software.
  \begin{center}
    {\bf Table 1: Computer Output}
    \medskip

    \begin{tabular}{crr} \hline
      Obs ($i$)  & $x_i$   &          Residual ($e_i$) \\ \hline
      1  & 0.0   &        0.04 \\
      2  & 4.1   &        0.11 \\
      3  & 5.1   &       -0.67 \\
      4  & 6.1   &        0.75 \\
      5  & $\ast$  &    $\ast$ \\ \hline
    \end{tabular}
  \end{center}
Fill in the two missing values (denoted by ``$\ast$'') in Table 1.  

\begin{bluebox}
As a consequence of the least squares method, the sum of residuals must be zero, here $\sum^{5}_{i=1} e_i = 0$.

So, to get $e_5$, we can find a value that would make the sum of residuals zero:

$$
0 = 0.04 + 0.11-0.67+0.75+e_5,
$$
So, 

$$
e_5 = -0.23
$$

Now, with only $x_5$ missing but its residual known, we can find its value as it can no longer freely vary (given the values of $x_1,\dots, x_4$ and $e_1,\dots,e_5$ is dictated). This is because we have a key relationship of residuals' orthogonality to the regressor, here we can express this relationship as $\sum^5_{i=1} x_ie_i = 0$

Solving for $x_5$,

$$
0 = 0.04(0) + 0.11(4.1) - 0.67(5.1) + 0.75 (6.1) - 0.23(x_5),
$$
gives

$$
x_5 = \frac{0 - 0.451 + 3.417 -4.575 }{-0.23} \approx 7.
$$

So, the missing entry is:

    \begin{tabular}{crr} \hline
      Obs ($i$)  & $x_i$   &          Residual ($e_i$) \\ \hline
      5  & 7  &    -0.23 \\ \hline
    \end{tabular}

\end{bluebox}

# Problem 3 

In a small-scale regression study, five observations on $Y$ were obtained corresponding to $X$ = 1, 4, 10, 11, and 14. Assume that $\sigma= 0.6$, $\beta_0 = 5$, and $\beta_1 = 3$.
  
## (a)  

What are the expected values of MSR and MSE here?

\begin{bluebox}
We can apply the equations from Slide 23 of the third course notes for this question: $\mathbb{E}[\text{MSE}] = \sigma^2$ and $\mathbb{E}[\text{MSR}] = \sigma^2 + \beta_1^2 \sum^n_{i=1}(X_i-\bar{X})^2$. Beforehand, we will want to know the value of $\bar{X}$, which can be easily calculated as $\bar{X} = \frac{1}{5}(1+4+10+11+14) = 8.$ Now, we have all known values needed for our calculations:

$$
\mathbb{E}[\text{MSE}] = \sigma^2 = 0.6^2 = 0.36, 
$$
and 

$$
\mathbb{E}[\text{MSR}] = 0.36 + 9\left( (1-8)^2 + (4-8)^2 + (10-8)^2 + (11-8)^2 + (14-8)^2 \right) = 1026.36.
$$
\end{bluebox}
  
## (b)  

For determining whether or not a regression relation exists, would it have been better or worse to have made the five observations at $X$ = 6, 7, 8, 9, and 10? Why? Discuss.

\begin{bluebox}
The $F$ test statistic is $F^\star = \frac{\text{MSR}}{\text{MSE}}$. For testing whether there is a nonzero regression slope, the null and alternative hypotheses are $H_0: \beta_1 = 0$ and $H_1: \beta_1 \neq 0$. Larger values of $F^\star$ tend to support the alternative hypothesis in favor of finding a regression relation. Spreading the values of $X$ is better as it provides additional opportunity for the test statistic to surpass a critical value given that a regression relationship exists. When the $X$ values are closer together, even if a regression relationship exists, it may be missed because of the deflation of the $\text{MSR}$. Given the power advantage, the original set would be more effective than this proposed set of $X$'s.
\end{bluebox}


\newpage

# Problem 4
A random sample of 100 paired observations $(X_i, Y_i)$, $i=1,2,\dots, 100$ was taken. Of the observations, 75 $(X_i,Y_i)$'s are $(2, 2)$ and 25 $(X_i,Y_i)$'s are $(1, 10)$. Suppose that we fit this data set with  a simple linear regression model $Y_i=\beta_0+\beta_1 X_i + \epsilon_i$ under the usual assumptions including Var$(\epsilon_i)=\sigma^2$.

## (a)  

What are the LS estimates $(b_0,b_1)'$ of $(\beta_0,\beta_1)'$? 

\begin{bluebox}
We only have two unique pairs for $(X_i, Y_i)$, so a straight line must be drawn which passes through both unique points, $(2,2)$ and $(1,10)$. Using basic algebra, we can identify this line by finding the slope and intercept:

\textbf{Slope:}

The slope can be calculated simply as $\frac{Y_1-Y_2}{X_1-X_2} = \frac{2-10}{2-1} = -8$.

\textbf{Intercept:}

The intercept of the LS fit line can be identified using the slope and one of our unique points:

$$
\hat{y} = -8x + \hat{\beta}_0 \implies \hat{\beta}_0 = \hat{y}+8x, \text{ so given observed point (2,2):}
$$
$$
\hat{\beta}_0 = 2+8(2) = 18.
$$

\textbf{Conclusion:}
Therefore, the LS coefficient estimates will be $(b_0,b_1)' = (18, -8)'$.
\end{bluebox}

## (b)  

What are the standard errors of $b_0$ and $b_1$, respectively? 

\begin{bluebox}
We would expect the standard errors for the coefficients to be exactly zero because all of the points lie on the LS fit line. To confirm, can calculate the standard errors for $\hat{\beta_0}$ and $\hat{\beta_1}$ using their respective formulas:

$$
\text{SE}(\hat{\beta_0}) = \sigma\sqrt{\frac{1}{n}+\frac{\bar{X}^2}{S_{XX}}}; \qquad \text{SE}(\hat{\beta_1 }) = \frac{\sigma}{\sqrt{S_{XX}}},
$$
But, we must make sure that all required quantities are known first. We know that $n=100$. We can calculate $\bar{X}$ as we are given the full data: $\bar{X} = \frac{25(1) + 75(2)}{100} = 1.75$. Next, we can obtain $S_{XX} = \sum{(X_i - \bar{X})^2} = 75(2-1.75)^2 + 25(1-1.75)^2 = 18.75$. Finally, we can estimate $\sigma$ with $\hat{\sigma} = \sqrt{\text{MSE}} = 0$ (MSE is zero because all points exist on the LS line for this data). So, 

$$
\text{SE}(\hat{\beta_0}) = 0\sqrt{\frac{1}{100}+\frac{1.75^2}{18.75}} = 0; \qquad \text{SE}(\hat{\beta_1 }) = \frac{\sigma}{\sqrt{S_{XX}}} = \frac{0}{\sqrt{18.75}} = 0.
$$


\end{bluebox}

## (c)  

What is the coefficient of determination $R^2$? 

\begin{bluebox}

The coefficient of determination $R^2$ can be calculated as $R^2 = \frac{SSR}{SSTO}$. In this model for this data set, every single point lays on the LS fit line, meaning that necessarily, $SSR=SSTO$. Since $SSTO$ is defined ($Y$ has two different values), $R^2 = 1$. 

\end{bluebox}

## (d)  

What is the hat matrix $H$? 

\begin{bluebox}
The SLR hat matrix is $ \mathbf{H} = \mathbf{X} ( \mathbf{X}' \mathbf{X})^{-1} \mathbf{X}'$. The design matrix for this problem is:

$$
\begin{pmatrix} 
  1 & x_1 \\\\ 
  1 & x_2 \\\\
  \vdots & \vdots \\\\
  1 & x_{100} \\\\
\end{pmatrix},
$$

Where 25 of the $x_i$'s are $1$ and the other 75 are $2$. In SLR, the components of $\mathbf{H}$ can be calculated as $h_{ij} = \frac{1}{n} + \frac{(x_i- \bar{X})(x_j - \bar{X})}{S_{XX}}$. This is helpful because we can avoid matrix multiplication and inversion computations, which are generally costly. In part (b), we went ahead and calculated $\bar{X}$ and $\S_{XX}$, which can now help us. Based on the fact that $X \in \mathbb{R}^{100 \times 2}$, the hat matrix $\mathbf{H} \in \mathbb{R}^{100 \times 100}$, quite a large matrix. However, we can simplify the matrix by recognizing that we will only have 3 unique combinations since we only have 2 unique $x$ values. These cases are:

For entries relating two points in $(2,2)$:

$$
h_{x=2, \; x=2} = \frac{1}{100} + \frac{(2 - 1.75)(2 - 1.75)}{18.75} = 0.01333
$$

For entries relating two points in $(1,10)$:

$$
h_{x=1, \; x=1} = \frac{1}{100} + \frac{(1 - 1.75)(1 - 1.75)}{18.75} = 0.04
$$

For entries relating mixed points $(2,2)$ and $(1,10)$:

$$
h_{x=1, \; x=2} = \frac{1}{100} + \frac{(1 - 1.75)(2 - 1.75)}{18.75} = 0
$$
So, the hat matrix is a 100 x 100 matrix with $h_{ii} = 0.04$ for the 25 observations with $x=1$ and $h_{ii} = 0.01333$ for the 75 observations with $x=2$. Otherwise, entries are $0$.

\end{bluebox}
  
## (e)  

What is an estimate of $\sigma^2$? 

\begin{bluebox}
We can estimate $\sigma^2$ as $\hat{\sigma}^2 = \text{MSE} = \frac{\text{SSE}}{n-2}$ in SLR, but since all our points fit on the LS line, we have no errors, i.e. $\text{SSE} = $. So, our estimate of $\sigma^2$ is zero.
\end{bluebox}


 
\newpage
# Problem 5

Work standards specify time, cost, and efficiency norms for the performance of work tasks. They are typically used to monitor job performance. In the distribution center of McCormick \& Company, Inc., data were collected to develop work standards for the time to assemble or fill customer orders. The table below contains data for a random sample of 9 orders.

\begin{center}
\begin{tabular}{c|ccccccccc}
  \hline
 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
  \hline
$X$ (number of items in the order) & 36 & 34 & 255 & 103 & 4 & 555 & 6 & 60 & 96 \\
  $Y$ (assemble time, minutes) & 27 & 15 & 71 & 35 & 8 & 60 & 3 & 10 & 10 \\
   \hline
\end{tabular}
\end{center}
```{r}
x <- c(36, 34, 255, 103, 4, 555, 6, 60, 96)
y <- c(27, 15, 71, 35, 8, 60, 3, 10, 10)
jobpf =  data.frame(x,y)
mod <- lm(formula = y ~ x)
summary(mod)
plot(x, y)
```
Using the above data of size $n=9$, answer the following questions.

## (a)  

Obtain the estimated regression function. Plot the estimated regression function and the data. Does a linear regression function appear to give a good fit here?

```{r}
ggplot(jobpf, aes(x = x, y = y)) +
  geom_point(
    size = 3,
    color = "wheat3",
    alpha = 0.8
  ) +
  geom_smooth(
    method = "lm",
    se = FALSE,
    color = "lightslateblue",
    linewidth = 1
  ) +
  labs(
    title = "Scatterplot with Linear Regression Fit",
    x = "x",
    y = "y"
  ) +
  theme_minimal(base_size = 13)
```

\begin{bluebox}

The estimated regression function is $\hat{Y} = 12.594 + 0.1094X_1$. 

Based upon the plot, the linear regression function appears to fit the data reasonably. Our regression outputs gave us reason to believe that a linear association exists ($\beta_1 \neq 0$) with the regression function providing meaningful explanatory power for the outcome variable ($R^2 \approx 0.64$). The sample size here is very small, and the perception of a linear relationship provided by the scatter plot may be subject to change under an increased sample size. While we can doubt whether this data provides enough information about the association between $X$ and $Y$, at present we do not have any reason to dispute a linear fit. 

\end{bluebox}

## (b)  

Obtain a point estimate of the expected assemble time for an order with $X = 70$ items.

\begin{bluebox}
Now, let's put our regression model to work to find out the fitted value for $X=70$:

$$
(\hat{Y}|X=70) = 12.594 + 0.1094(70) = 20.252
$$
Our point estimate for $Y$ given $X=70$ in this simple linear regression model is $20.252$.
\end{bluebox}

## (c) 

```{r}
# Calculate MSE and S_XX for CI computation:
## MSE calculation:
SSE <- sum(residuals(mod)^2)
n <- length(y)
MSE <- SSE / (n - 2)
cat("MSE:", round(MSE, 3))

## S_XX calculation:
SXX <- sum((x - mean(x))^2)
cat("S_XX:", round(SXX,3))

# Confidence interval calculation
SE_beta1 <- sqrt(MSE / SXX)

## Check if manual calculation matches model:
R_SE_beta1 <- summary(mod)$coefficients["x", "Std. Error"]
cat("Calculated SE_beta_1:", SE_beta1, "; R-given SE_beta_1:", R_SE_beta1)

## Critical t value:

crit_t <- qt(0.975, 7) # 9 data points -> n-2 = 7 df

upper_95_bound_beta_1 <- 0.1094 + crit_t*SE_beta1
lower_95_bound_beta_1 <- 0.1094 - crit_t*SE_beta1
print(paste0("95% CI for Beta_1: (", round(lower_95_bound_beta_1,3), ",", 
             round(upper_95_bound_beta_1,3), ")"))
```

Estimate $\beta_1$ with a 95 percent confidence interval. Interpret your interval estimate.

\begin{bluebox}
We can provide a $95\%$ confidence interval for $\beta_1$ by starting with our model-provided point estimate for $\beta_1$, $\hat{\beta_1} = 0.1094$ and building the outer bounds with the equation $
\hat{\beta_1} \pm t_{1-\frac{\alpha}{2}, \enspace n-2} \times \text{se} \left[ \hat{\beta_1} \right], $
where $\text{se} \left[ \hat{\beta_1} \right] = \sqrt{\frac{\text{MSE}}{S_{XX}}}$. We can calculate $S_{XX}$ and $\text{MSE}$ to provide those quantities to our confidence interval calculation, giving interval $(0.035, 0.183)$. Since we confidence interval does not contain zero, we can interpret this as evidence at the $95\%$ confidence level (in a two-sided scenario) that the regression coefficient $\beta_1$ is not equal to zero, and that therefore, a regression relationship exists between $X$ and $Y$ in this model. 

\end{bluebox}

## (d)  

Conduct a $t$ test to decide whether or not there is a linear association between number of items in an order ($X$) and the assemble time ($Y$). Use a level of significance of .05. State the alternatives, decision rule, and conclusion. What is the p-value of the test?



\begin{bluebox}
In the prior sub-question, we used confidence intervals to express the idea of identifying whether there is a linear association between $X$ and $Y$. Now, we can approach that question again from a hypothesis testing perspective. Our hypotheses will be:

$$
H_0 : \beta_1 = 0,
$$

$$
H_1 : \beta_1 \neq 0,
$$
As a slope ($\beta_1$) of $0$ would suggest no linear association. We will use a significance level $\alpha$ of $0.05$. Therefore, our decision rule will be to reject the null hypothesis which stated $\beta_1 = 0$ if we obtain a p-value of less than $0.05$ which is equivalent to rejected the null hypothesis if our test statistic exceeds $t_{0.975, \enspace 7}$ as we will use a two-sided test to mirror the construction of the confidence interval. 

The test results are automatically included in R's model summary; for $\hat{\beta_1}$, we get a t test statistic of 3.495 (exceeding the critical value) and a p-value of 0.0101, below the significance level. Therefore, we reject the null hypothesis and can conclude at a $0.05$ significance level that the linear regression coefficient for this SLR model is nonzero, and therefore, that there is a significant linear association between $X$ and $Y$ in this model. 

\end{bluebox}

## (e)  

Estimate the mean assemble time for orders with the following numbers of items: $X = 70$ and $X=100$. Use separate 99 percent confidence intervals. Interpret your results.

```{r}
# Calculate bar{x}:
x <- c(36, 34, 255, 103, 4, 555, 6, 60, 96)
print(paste0("xbar = ", round(mean(x), 2)))

# Compute Confidence Intervals:

## Critical t-value
crit_t_99ci_7df <- qt(0.995, 7)

## Fitted on X=70:
fitted_Y_x70 <- 20.252
se_hat_Y_x70 <- sqrt( 
  MSE*( (1/9) + ((mean(x) - 70)^2/SXX) )
  )
lower_bound_99ci_x70 <- fitted_Y_x70 - crit_t_99ci_7df*se_hat_Y_x70
upper_bound_99ci_x70 <- fitted_Y_x70 + crit_t_99ci_7df*se_hat_Y_x70
print(paste0("99% CI for Fitted Value of Y given X=70: (", 
             round(lower_bound_99ci_x70, 3) , ", ", 
             round(upper_bound_99ci_x70, 3) , ")"))

## Fitted on X=100: 
fitted_Y_x100 <- 23.534
se_hat_Y_x100 <- sqrt( 
  MSE*( (1/9) + ((mean(x) - 100)^2/SXX) )
  )
lower_bound_99ci_x100 <- fitted_Y_x100 - crit_t_99ci_7df*se_hat_Y_x100
upper_bound_99ci_x100 <- fitted_Y_x100 + crit_t_99ci_7df*se_hat_Y_x100
print(paste0("99% CI for Fitted Value of Y given X=100: (", 
             round(lower_bound_99ci_x100, 3), ", ", 
             round(upper_bound_99ci_x100, 3), ")"))
```

\begin{bluebox}
Now, instead of constructing a confidence interval for the linear regression coefficient, we are seeking confidence intervals for fitted values of two specific order sizes, $70$ and $100$. We will be building $99\%$ confidence intervals, which will end up being relatively wide compared to other choices of confidence intervals. We can build these confidence intervals by beginning with the fitted values and using the corresponding standard errors to expand into an interval. The CI for a fitted value in an SLR model is $(\hat{Y}|X=x) \pm t_{0.995, \enspace7} \times \sqrt{\text{MSE} \left( \frac{1}{n}+\frac{(\bar{X}-X_h)^2}{S_{XX}}\right)}$. The fitted values for $X=70$ and $X=100$ are $(\hat{Y}|X=70) = 12.594 + 0.1094(70) = 20.252$ and $(\hat{Y}|X=100) = 12.594 + 0.1094(100) = 23.534$, respectively. To compute our confidence intervals, we must know $\bar{X}$, which is $126.67$

\textbf{Confidence Interval ($99\%$) for $\hat{Y_{X=70}}$:}

The 99$\%$ confidence interval for $\hat{Y_{X=70}}$ is given by:

$$
(\hat{Y}|X=70) \pm t_{0.995, \enspace7} \times \sqrt{\text{MSE} \left( \frac{1}{9}+\frac{(126.67-70)^2}{S_{XX}}\right)} = (0.862, 39.642)
$$

\textbf{Confidence Interval ($99\%$) for $\hat{Y_{X=100}}$:}

The 99$\%$ confidence interval for $\hat{Y_{X=100}}$ is given by:

$$
(\hat{Y}|X=100) \pm t_{0.995, \enspace7} \times \sqrt{\text{MSE} \left( \frac{1}{9}+\frac{(126.67-100)^2}{S_{XX}}\right)} = (4.952, 42.116)
$$

We can interpret this confidence interval as a range of values that we expect the mean of the assembly time for these respective order sizes ($X=70,100$) to exist in at a 99$\%$ confidence level.
\end{bluebox}

## (f)  

The next order has 30 items. Obtain a 99 percent prediction interval for the assemble time for this order. Interpret your prediction interval.

```{r}
# Build prediction interval for X=30

fitted_val_30 <- predict(mod, newdata = data.frame(x = 30))
se_pred30 <- sqrt(
  MSE * (1 + (1/9) + (30-mean(x))^2/SXX)
)

lower_99_ci_bound_pred30 <- fitted_val_30 - crit_t_99ci_7df * se_pred30
upper_99_ci_bound_pred30 <- fitted_val_30 + crit_t_99ci_7df * se_pred30

print(paste0("99% Pred. Int. for Y for new obs. X=30: (", 
             round(lower_99_ci_bound_pred30, 3), ", ", 
             round(upper_99_ci_bound_pred30, 3), ")"))
```

\begin{bluebox}
The spirit of this question is similar to the last, but this time we are interested in building a \textit{prediction interval}, which is formulated in a slightly different way to a typical confidence interval for the mean response and which has a different interpretation. This time, our interval can be constructed under the structure $(\hat{\beta}_0 + \hat{\beta}_1 X_{\text{new}}) \pm t_{1-\frac{\alpha}{2},\;n-2} \sqrt{ \text{MSE} \left( 1 + \frac{1}{n} + \frac{(X_{\text{new}} - \bar{X})^2}{S_{XX}} \right)}$.

Our $99\%$ prediction interval for $X=30$ is therefore:

$$
\left(12.594 + 0.10936(30)\right) \pm t_{0.995, \; 7} \sqrt{ \text{MSE} \left( 1 + \frac{1}{9} + \frac{(30 - 126.67)^2}{S_{XX}} \right)} = (-43.059, 74.809)
$$

Since we cannot have negative time for filling an order, the interpretability of our prediction interval suffers. We obtain this negative bound because of the large interval selection, low sample size, and observed variance large enough to push the bound negative (there is no constraint for having strictly positive CI/PI bounds in SLR). We can still say with $99\%$ confidence that a future observation with $X=30$ will fall in our prediction interval. 
\end{bluebox}

## (g)  

Set up the ANOVA table. Which elements are additive?

```{r}
# Use anova() to get info:
anova_table <- anova(mod)

# Build full SLR ANOVA table:
SSR <- anova_table["x", "Sum Sq"]         # Sum of Squares - Regression
SSE <- anova_table["Residuals", "Sum Sq"] # Sum of Squares - Error
SSTO <- SSR + SSE                     # Total Sum of Squares

df_SLR <- anova_table["x", "Df"]          # = 1
df_E   <- anova_table["Residuals", "Df"]  # = n - 2
df_T   <- df_SLR + df_E               # = n - 1

MSR <- SSR / df_SLR # Mean Square for Regression
MSE <- SSE / df_E   # Mean Square Error

F_star <- MSR / MSE #
p_val  <- pf(F_star, df_SLR, df_E, lower.tail = FALSE) #

data.frame( # Table form
  Source = c("SLR", "Error", "Corrected Total"),
  df = c(df_SLR, df_E, df_T),
  SS = c(SSR, SSE, SSTO),
  MS = c(MSR, MSE, NA),
  F = c(F_star, NA, NA),
  `P-value` = c(p_val, NA, NA)
)
```

\begin{bluebox}
We can build the ANOVA table using the built-in R ``anova" function to get most of the important values and fill in the true, full ANOVA table that we are seeking in the style of the ANOVA table on Slide 22 in Note 3:

\[
\begin{array}{lcccccc}
\hline
\text{Source of Variation} & \text{df} & \text{SS} & \text{MS} & F & \text{P-value} \\
\hline
\text{SLR} & 1 & 3017.122 & 3017.122 & 12.214 & 0.0101 \\
\text{Error} & 7 & 1729.100 & 247.014 &  &  \\
\text{Corrected Total} & 8 & 4746.222 &  &  &  \\
\hline
\end{array}
\]

For additivity, we have some key relationships. First, $\text{SSTO} = \text{SSR} + \text{SSE}$. Second, degrees of freedom are additive in SLR: the degrees of freedom from SLR are 1, and the degrees of freedom from error are $n-2$. Together, they sum to $n-1$.
\end{bluebox}

## (h) 

Conduct an $F$ test to decide whether or not there is a linear association between the assemble time and the number of items; control the $\alpha$ risk at .05. State the alternatives, decision rule, and conclusion.

\begin{bluebox}
Now, we would like to use an $F$ test for linear association between $X$ and $Y$. We can execute the test using R, but let's explicitly set up our test here:

We have the hypotheses:

$$
H_0: \beta_1 = 0,
$$

$$
H_A: \beta_1 \neq 0.
$$

We will set our significance level $\alpha$ to $0.05$, and therefore we will reject our null hypothesis if we find the p-value of our test to be below this threshold.

We do not have to write any more code to answer this problem as, for simple linear regression, the test of the model that we observe in the ANOVA table is identical to the test we are looking to perform. Therefore, we obtained a p-value of $0.0101$ for our test, below the pre-specified significance level threshold. Therefore, we reject the null hypothesis which suggested that no linear association exists between the number of items in an order and the order preparation time.

\end{bluebox}

## (i)  

Obtain the $t^\star$ statistic for the test in part (h) and demonstrate numerically its equivalence to the $F^\star$ statistic obtained in part (h).

\begin{bluebox}
In simple linear regression, the t- and F-test statistics yield identical results. Indeed, in parts (d) and (h), we can observe that the p-values for the two tests for $H_0: \beta_1 = 0$ are identical.

Numerically, we can deduce why these test results coincide exactly. Under this null hypothesis:

$$
t^\star = \frac{\hat{\beta_1 }-0 }{\text{SE}(\hat{\beta_1})} = \frac{\hat{\beta_1 } }{\sqrt{\frac{MSE}{S_{XX}}}} = 3.495
,
$$

$$
F^\star = \frac{MSR}{MSE} = \frac{\frac{SSR}{1}}{MSE} = \frac{SSR}{MSE} = \frac{\hat{\beta_1^2}S_{XX}}{MSE} = \frac{\hat{\beta_1^2}}{\frac{MSE}{S_{XX}}} = (t^\star)^2 = 12.214,
$$
As we can see, the test statistics are composed of the same components. In SLR, $T \sim t_\nu$ and $T^2 \sim F_{1, \nu}$. So, $t^\star \sim t_{n-2}$ and $(t^\star)^2 = F^\star \sim F_{1, \nu}$. The tests are equivalent. Note that SLR is a special case, and these relationships are not necessarily true in general for other regression scenarios.

\end{bluebox}

## (j)  

Calculate $R^2$ and $r$. What proportion of the variation in $Y$ is accounted for by introducing $X$ into the regression model?

\begin{bluebox}
\textbf{Coefficient of Determination Calcualtion:}

We can calculate $R^2$ using either the $SSR$ or $SSE$, $R^2 = \frac{SSR}{SSTO} = 1 - \frac{SSE}{SSTO}$. Let's go with the $SSR$-based formula for this example:

$$
R^2 = \frac{3017.122}{4746.222} = 0.636.
$$

Our coefficient of determination is $0.636$, which suggests that $63.6\%$ of the variation of $Y$ can be captured by variation in $X$ introduced by using this regression model. 

\textbf{Correlation Calculation:}

Now, we can calculate the correlation coefficient, $r$, which is identically the square root of $R^2$ for the simple linear regression model, so $\sqrt{R^2} = \sqrt{0.636} = 0.797.$

\end{bluebox}

## (k) 

It took 50 minutes to assemble an order. Obtain a 95 percent confidence interval for the number of items in this order using both the delta and exact methods. Interpret your confidence interval.

```{r}
# Get Critical t value:
crit_t_95_2side_7 <- qt(0.975, 7)
cat("Critical t (0.95%, two-sided, 7df):", crit_t_95_2side_7)

# Check Delta Method Calculation:
Y0 <- 50
b0  <- coef(mod)[1]
b1  <- coef(mod)[2]
n   <- length(mod$fitted.values)
Xbar <- mean(mod$model[,2])
SXX  <- sum((mod$model[,2] - Xbar)^2)
MSE <- summary(mod)$sigma^2
X_hat <- (Y0 - b0) / b1
se_X_hat <- sqrt(
  (MSE / b1^2) *
  (1 + 1/n + (X_hat - Xbar)^2 / SXX)
)
CI_delta <- c(
  lower = X_hat - crit_t_95_2side_7 * se_X_hat,
  upper = X_hat + crit_t_95_2side_7 * se_X_hat
)
list(
  X_hat = X_hat,
  SE = se_X_hat,
  t_crit = crit_t_95_2side_7,
  CI_delta = CI_delta
)

# Exact Method Calculation:
b1    <- coef(mod)[2]
X_hat <- X_hat
sigma2_hat <- MSE
tcrit <- qt(0.975, n - 2)
A <- b1^2 - (sigma2_hat * tcrit^2) / SXX
B <- -2 * b1^2 * X_hat +
     2 * (sigma2_hat * tcrit^2 * Xbar) / SXX
C <- b1^2 * X_hat^2 -
     sigma2_hat * tcrit^2 *
     (1 + 1/n + (Xbar^2 / SXX))
roots <- polyroot(c(C, B, A))
roots_real <- Re(roots[abs(Im(roots)) < 1e-8])
X_exact_CI <- sort(roots_real)
X_exact_CI
```

\begin{bluebox}

We have to focus on using prediction intervals for his problem as $Y=50$ as an assembly time represents purely a realization of $Y$, not necessarily a mean response for some given value of $X = x$.

\textbf{Delta Method:}

The Delta Method can be employed here to essentially ``turn the fitted model on its head" and find an estimate for $(X \; | \; y=50)$ and build out appropriate standard errors to produce an approximate 95\% confidence interval for the number of items in the order. So, instead of finding a fitted value as $y=\hat{\beta_0} + \hat{\beta_1}X$, we are actually getting a point estimate through $\hat{X} = \frac{Y-\beta_0}{\beta_1}$. In our problem, this point estimate is:

$$
\hat{X} = \frac{50-12.594}{0.10936} = 342.04 \approx 342\text{ items}.
$$

Now, we need our critical t-value, which will be $t_{0.975, \; 7}$ because we are building a two-sided, 95% CI. In combination, we must find the standard error of our point estimate. Using the Delta Method with the prediction interval setup, we are given (by Slide 44 of Note 3): 

$$
\widehat{se}(\hat{X}) = \sqrt{\frac{\text{MSE}}{\hat{\beta_1^2}} 
\left[ 
1 + \frac{1}{n} + \frac{(\hat{X}-\bar{X})^2}{S_{XX}}
\right]},
$$
With this example's values, this can be calculated as:

$$
\widehat{se}(342.04) = \sqrt{\frac{274.014}{{0.1094^2}} 
\left[ 
1 + \frac{1}{9} + \frac{(342.04-27.67)^2}{252290}
\right]} = 185.49,
$$
So the Delta Method confidence interval is:

$$
\hat{X} \; \pm \; t_{0.975, \; 7} \times \widehat{se}(342.04) = 342.04 \pm 2.364\times185.49 = (-44.43, 728.53)
$$

While we cannot have a negative number of items in an order, we can still be sure that the number of items in an order that takes $50$ minutes to fill will be in our confidence interval with 95\% confidence. It may be useful to round our CI to integer values as the number of items in an order is an integer value. The rounded CI will be $(-45, 729)$.

\textbf{Exact Method:}

In the exact method, we are seeking two $x$-values: one for where $Y=50$ intersects the upper prediction bound for $Y$, and one for where $Y=50$ intersects the lower prediction band for $Y$. We can make this happen by solving:

$$
\hat{\beta}_1^2 (X^\star - \hat{X}_0)^2 = \hat{\sigma}^2 t^2_{1-\alpha/2, n-2}\left( 1 + \frac{1}{n} + \frac{(X^\star - \bar{X})^2}{S_{XX}} \right).
$$

\textit{Continued on next page}
\end{bluebox}

\newpage 

\begin{bluebox}
The roots we identify will be the bounds of the exact interval. We will calculate the roots using R to avoiding excessive rounding errors. The confidence interval is $(-32.13, 1078.29)$. Once again we run into an issue with encountering a negative lower bound. We cannot have negative items in the order. However, we still can expect the true number of items for an order taking $50$ minutes to fall in our CI at a $95\%$ confidence level. We should realize that our confidence intervals end up being extremely broad in this example. Seeking more data at well-placed $X$ values may help pave the road to narrower confidence intervals in the future. 

\end{bluebox}

## (l) 

In part (k), is criterion in page 52 of Note 3 satisfied regarding the appropriateness of the approximate confidence interval?

```{r}
# Compute ratio:
ratio <- (crit_t_95_2side_7^2*MSE)/(b1^2*SXX)
cat("Ratio:", ratio)
```

\begin{bluebox}
Let's check on the size of: 
$$
\frac{t^2_{1-\alpha/2, \; n-2} \text{MSE}}{\hat{\beta_1^2}S_{XX}}
$$
To evaluate the appropriateness of the approximate confidence interval. This ratio ends up being approximately $0.458$. While this value isn't very large, we may begin to experience the delta method approximation working more poorly than we would like it to. This criterion is only somewhat satisfied.

\end{bluebox}

\newpage

# Problem 6 

In fitting a simple linear regression on a data set with mean response $\bar{Y}=0.9112$ and covariate $X$ ranging from -2.224 to  2.441, the following partial R outputs are obtained.

```r
Call:
lm(formula = y ~ x)
Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)   0.9026     0.1926
x             0.3776     0.2192

Analysis of Variance Table
Response: y
          Df Sum Sq Mean Sq F value  Pr(>F)
x          1   11.0
Residuals 98

Residual standard error: 1.925 on 98 degrees of freedom
```

## (a) 

Find the sample size $n$, SSE, $\hat\sigma^2$, and R-square $R^2$.
  
\begin{bluebox}
We can use the information given in this R output to identify all of these quantities:

\textbf{Finding $n$:}

In the last line of the output we observe that the residuals have $98$ degrees of freedom. We know from the structure of SLR that $\beta_0$ and $\beta_1$ each consume a degree of freedom for the residuals, and therefore the original sample size was $100$. 

\textbf{Finding $\hat{\sigma}^2$:}

We will temporarily jump ahead of finding $SSE$ as identifying $\hat{\sigma}^2$ will make finding $SSE$ much easier. The estimated variance of the error term $\epsilon$, $\hat{\sigma}^2$, can be identified through the formula $\text{Residual standard error}= \sqrt{\hat{\sigma}^2}$. So, $(\text{Residual standard error})^2 = \hat{\sigma}^2 = 1.925^2 \approx 3.7056.$

\textbf{Finding $SSE$:}

Now, we can find the SSE using the relationship $\hat{\sigma}^2 = \frac{SSE}{n-2}$, rearranging and solving to obtain $SSE = (n-2)\hat{\sigma}^2 = 98\times3.7056 \approx 363.1512$

\textbf{Finding $R^2$:}

We can find $R^2$ using its $R^2 = \frac{SSR}{SSTO}$ construction. We found the error sum of squares, $SSE = 363.1512$, and we are given the residual sum of squares, $11.0$, whose sum, the total sum of squares, is $363.1512+11.0 = 374.2$. So, $R^2 = \frac{11.0}{374.2} \approx 0.0294$.

\end{bluebox}
  
## (b)  

Find the p-value for testing $H_0:\beta_0=0$ versus $H_1:\beta_0\neq0$, and the p-value for testing $H_0:\beta_1=0$ versus $H_1:\beta_1 < 0$.

```{r}
# p-value calculations
## First test
test_1_p_val <- 2 * (1 - pt(abs(4.69), df = 98))
## Second test
test_2_p_val <- pt(1.72, df = 98)

print(paste0("The p-value for test 1 was: ", round(test_1_p_val, 7), 
            ". The p-value for test 2 was: ", round(test_2_p_val, 3), "."
            ))
```

\begin{bluebox}
Now, let's calculate the p-values for these hypotheses. We will need our test statistic to compute the p-values, so we can obtain those using formulas $t_0 = \frac{\hat{\beta}_0-\beta_{0,0}}{\text{SE}(\hat{\beta}_0)} = \frac{0.9026}{0.1926} = 4.686$ and $t_1 = \frac{\hat{\beta}_1-\beta_{1,0}}{\text{SE}(\hat{\beta}_1)} = \frac{0.3776}{0.2192} = 1.723$

\textbf{Test of $H_0:\beta_0=0$ versus $H_1:\beta_0\neq0$: }

The p-value for this test is equivalent to the probability that the absolute value of the $t^\star$ test statistic is less than or equal or the absolute value of a critical value $t_{n-2}$.

Therefore, we can calculate $\text{p-value} = P(|t_{n-2}| \le |t^\star|) = P(|t_{98}| \le 4.686 = 8.8 \times 10^{-6})$.

\textbf{Test of $H_0:\beta_1=0$ versus $H_1:\beta_1 < 0$: }

The p-value for this test is equivalent to the probability that the test statistic is greater than the critical value. 

So, we can calculate $\text{p-value} = P(t_{n-2} < t^\star) = P(t_{98} < 1.723) = 0.956$ 
\end{bluebox}

## (c)  

Find the sample coefficient of variation for the covariate $X$, CV$_X$ (see page 45 of Note 1 for the definition).
  

  For the same data set, we have the following
\begin{verbatim}
> CI<-predict(fit, se.fit=TRUE, interval = c("confidence"))
> head(cbind(y,CI$fit))
            y       fit        lwr      upr
1  2.69001380 1.3794827  0.7183901 2.040575
2 -0.58617658 0.7794220  0.3683444 1.190500
3 -0.09899989 1.4047227  0.7196928 2.089753
\end{verbatim}
 
\begin{bluebox}
According to page 45 of Note 1, the coefficient of variation for $X$ can be defined as: $ CV_X = (S_{XX} /n)^{1/2}/|\bar{X}|$. We know that $n=100$, but we need to find $S_{XX}$ and $\bar{X}$ to round out our computation. 

To find $S_{XX}$, we can use the relation that $S_{XX} = \frac{\hat{\sigma}^2}{\text{SE}(\hat{\beta}_1)^2}$. We know that $\hat{\sigma}^2 = 3.7056$ from our work in Part (a) and that $\text{SE}(\hat{\beta}_1) = 0.2192$ from the outputs provided by the question. So, $S_{XX} = \frac{3.7056}{0.2192^2} = 71.122$.

To find $\bar{X}$, we should realize that we are given $\bar{Y}=0.9112$ by the problem, and that in SLR, we can be confident that $\bar{Y} = \hat{\beta}_0 + \hat{\beta}_1\bar{X}$. Rearranging and plugging in values, we can acquire: $\bar{X} = \frac{\hat{Y} - \hat{\beta}_0 }{\hat{\beta}_1} = \frac{0.9112-0.9026}{0.3776} = 0.0228$. 

Finally, we can acquire $CV_{X}$ as $CV_X = \frac{ \sqrt{\frac{71.122}{100}} }{ |0.0228| } \approx 37$.
\end{bluebox}

## (d)  

For $X$ with the same value as in the first observation (the first line of the output with $Y=2.69001380$), find the SE of estimating the mean response and the SE of $Y_{new} -\hat{Y}_{new}$.

```{r}
# Find critical value of t_0.975,98
crit_t_6d <- qt(0.975, 98)

print(paste0("Critical t: ", round(crit_t_6d, 5)))
```

\begin{bluebox}

\textbf{SE of estimating the mean response:}

The $\text{SE}$ for estimating the mean response can be identified using the confidence interval for the mean response at the value of x where $Y=2.69001380$. This confidence interval was constructed as $\hat{y}_1 \pm t_{0.975, \; 98}\text{SE}(\hat{Y}(x_{y=2.69}))$. Half the width of the confidence interval would represent $t_{0.975, \; 98}\text{SE}(\hat{Y}(x_{y=2.69}))$. 

That quantity is $\frac{2.040575-0.7183901}{2} = 0.6611$. Dividing by $t_{0.975, \; 98}$, $\text{SE}(\hat{Y}(x_{y=2.69}) = \frac{0.6611}{t_{0.975, \; 98}} = \frac{0.6611}{1.98447} = 0.3331$.

\textbf{SE of $Y_{new} - \hat{Y}_{new}$:}

For a new observation, the standard error is:

$$
\text{SE}(Y_{new}-\hat{Y}_{new}) = \hat{\sigma} 
\sqrt{ 
1 + \frac{1}{n} + \frac{(x_0-\bar{X})^2}{S_{XX}}
},
$$
Which can be written in terms of the standard error we found previously:

$$
\text{SE}(Y_{new}-\hat{Y}_{new}) = 
\sqrt{ 
\hat{\sigma}^2 + \text{SE}(\hat{Y}(x_0))^2 
} = \sqrt{3.7056 + 0.3331^2} = 1.954,
$$
\end{bluebox}

## (e) 

For a single observation with $X=1$, find the 95\% prediction interval for the response.

\begin{bluebox}
Our prediction interval can be constructed according to the formula 

$$
\hat{Y}(x_0) \pm t_{0.975, \; n-2} \times \text{SE}(Y_{new}-\hat{Y}_{new}).
$$
So, we need to compute a fitted value and standard error to build our interval:

\textbf{Fitted Value:}

$\hat{Y}(1) = 0.9026+0.3776(1)=1.2802$

\textbf{Standard Error:}

$$
\text{SE}(Y_{new}-\hat{Y}_{new}) = 1.925 
\sqrt{ 
1 + \frac{1}{100} + \frac{(1-0.0228)^2}{71.122}
}
= 1.947,
$$

\textbf{Prediction interval:}

So, knowing that $t_{0.975, \; 98} = 1.98447$ the prediction interval can be formed as $1.2802 \; \pm \; 1.947 \; \times \; 1.98447  = (-2.584 , \; 5.144)$.

\end{bluebox}
           