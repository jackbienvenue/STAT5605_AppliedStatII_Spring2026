---
title: "STAT 5605 Homework 2"
subtitle: "February 5, 2026"
author: "Jack Bienvenue"
output:
  bookdown::pdf_document2:
    includes:
      in_header: preamble.tex
    latex_engine: pdflatex
    keep_tex: true 
header-includes:
  - \usepackage{titling}
  - \setlength{\droptitle}{1em} 
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)

invisible(lapply(c( "ggplot2"), 
  function(pkg) suppressPackageStartupMessages(library(pkg, 
                                                character.only = TRUE))))
```


# Problem 1 

The least squares regression line for a given set of data with a sample size of $n=20$ is $\hat{Y}=-42 + 0.9 X$ (i.e., $b_0=-42$ and $b_1=0.9$). The MSE of the fitted simple linear regression (SLR) model is 0.14, the standard error of $b_1$ (i.e., $se(b_1)$) is 0.016, and the upper limit of a 95\% predictive interval at $X_{new}=222$ is 165.2.  Further, we assume that the error term has a normal distribution. Answer the following questions.

```{r}
# Calculate critical value for t:
t_crit_1a <- qt(0.975, df=18)
cat("t critical value:", t_crit_1a)
```

## (a) 

What is the standard error of the fitted value of $Y$ at $X=222$ (i.e., $se(\hat{Y})$)?

\begin{bluebox}
We can calculate $\text{se}(\hat{Y})$ by using the formula $\text{se}(\hat{Y}_{X=222}) = \sqrt{\text{MSE}\left(\frac{1}{n}+\frac{(\bar{X}-222)^2}{S_{XX}}\right)}$. So, we must find values for $\bar{X}$, $S_{XX}$. 

The value $S_{XX}$ is not immediately available from the given information, but it can be retrieved from what we know about $0.016 = \text{se}(b_1) = \sqrt{\frac{\text{MSE}}{S_{XX}}}$. Given $\text{MSE} = 0.14$, we can rearrange the equation to become $S_{XX} = \frac{\text{MSE}}{(\text{se}(b_1))^2} = \frac{0.14}{0.016^2} = 546.875$.

To find $\bar{X}$ to calculate the standard error we are seeking, we will have to work backwards from the upper value of the $95\%$ predictive interval we are given. The upper end of this predictive interval is defined by the formula $\text{Upper Bound} = \hat{Y}(222)+t_{0.975, n-2}\sqrt{\text{MSE}\left(1+\frac{1}{n}+\frac{(222-\bar{X})^2}{S_{XX}} \right)}$. To calculate the value of $\bar{X}$, we can compute $\hat{Y}$ given $X=222$ and input our known values into the equation and rearrange:

$$
\hat{Y}(222) = -42+0.9(220) = 157.8
$$

Now, we can approach $\bar{X}$:

$$
165.2 = 157.8 + 2.101\sqrt{0.14\left(1+\frac{1}{20} + \frac{(222-\bar{X})^2}{546.875} \right)} = 
$$
$$
\frac{\left(\frac{165.2-157.9}{2.101}\right)^2}{0.14} - 1.05 = \frac{(222-\bar{X})^2}{546.875} = 
$$

$$
= \bar{X} = 222 - \sqrt{ (546.875)\left( \frac{\left(\frac{165.2-157.8}{2.101}\right)^2}{0.14} - 1.05 \right) } = -44.655
$$

Now, the standard error of $\hat{Y}$ at $X=222$ can finally be calculated:

$$
\text{se}(\hat{Y}_{X=222}) = \sqrt{\text{0.14}\left(\frac{1}{20}+\frac{(-44.655-222)^2}{546.875}\right)} = 4.2707
$$

FIX!!!!!!!!!!!!!!!!!!!!!!!!!!!

\end{bluebox}

## (b) 

What are the predicted value of $Y_{new}$ and its standard error at $X_{new}=222$?

\begin{bluebox}
a
\end{bluebox}

## (c) 

How much does use of this SLR model reduce the uncertainty in predicting the response variable $Y$?

\begin{bluebox}
a
\end{bluebox}

\newpage

# Problem 2

An analysis is performed to study the relationship between a response variable $Y$ and a single explanatory variable $X$. The model guiding the analysis is:

$$
  Y_i=\beta_0+\beta_1 X_{i}+ \epsilon_i, 
$$ 
where the $\epsilon_i$ are independently and identically distributed as $N(0,\sigma^2)$ and $\sigma^2$ is unknown. The data with a sample size of $n=5$ were analyzed using PROC REG in SAS (or function lm in R) and Table 1 below shows the partial output produced by the software.
  \begin{center}
    {\bf Table 1: Computer Output}
    \medskip

    \begin{tabular}{crr} \hline
      Obs ($i$)  & $x_i$   &          Residual ($e_i$) \\ \hline
      1  & 0.0   &        0.04 \\
      2  & 4.1   &        0.11 \\
      3  & 5.1   &       -0.67 \\
      4  & 6.1   &        0.75 \\
      5  & $\ast$  &    $\ast$ \\ \hline
    \end{tabular}
  \end{center}
Fill in the two missing values (denoted by ``$\ast$'') in Table 1.  

\begin{bluebox}
As a consequence of the least squares method, the sum of residuals must be zero, here $\sum^{5}_{i=1} e_i = 0$.

So, to get $e_5$, we can find a value that would make the sum of residuals zero:

$$
0 = 0.04 + 0.11-0.67+0.75+e_5,
$$
So, 

$$
e_5 = -0.23
$$

Now, with only $x_5$ missing but its residual known, we can find its value as it can no longer freely vary (given the values of $x_1,\dots, x_4$ and $e_1,\dots,e_5$ is dictated). This is because we have a key relationship of residuals' orthogonality to the regressor, here we can express this relationship as $\sum^5_{i=1} x_ie_i = 0$

Solving for $x_5$,

$$
0 = 0.04(0) + 0.11(4.1) - 0.67(5.1) + 0.75 (6.1) - 0.23(x_5),
$$
gives

$$
x_5 = \frac{0 - 0.451 + 3.417 -4.575 }{-0.23} \approx 7.
$$

So, the missing entry is:

    \begin{tabular}{crr} \hline
      Obs ($i$)  & $x_i$   &          Residual ($e_i$) \\ \hline
      5  & 7  &    -0.23 \\ \hline
    \end{tabular}

\end{bluebox}

# Problem 3 

In a small-scale regression study, five observations on $Y$ were obtained corresponding to $X$ = 1, 4, 10, 11, and 14. Assume that $\sigma= 0.6$, $\beta_0 = 5$, and $\beta_1 = 3$.
  
## (a)  

What are the expected values of MSR and MSE here?

\begin{bluebox}
We can apply the equations from Slide 23 of the third course notes for this question: $\mathbb{E}[\text{MSE}] = \sigma^2$ and $\mathbb{E}[\text{MSR}] = \sigma^2 + \beta_1^2 \sum^n_{i=1}(X_i-\bar{X})^2$. Beforehand, we will want to know the value of $\bar{X}$, which can be easily calculated as $\bar{X} = \frac{1}{5}(1+4+10+11+14) = 8.$ Now, we have all known values needed for our calculations:

$$
\mathbb{E}[\text{MSE}] = \sigma^2 = 0.6^2 = 0.36, 
$$
and 

$$
\mathbb{E}[\text{MSR}] = 0.36 + 9\left( (1-8)^2 + (4-8)^2 + (10-8)^2 + (11-8)^2 + (14-8)^2 \right) = 1026.36.
$$
\end{bluebox}
  
## (b)  

For determining whether or not a regression relation exists, would it have been better or worse to have made the five observations at $X$ = 6, 7, 8, 9, and 10? Why? Discuss.

\begin{bluebox}
The $F$ test statistic is $F^\star = \frac{\text{MSR}}{\text{MSE}}$. For testing whether there is a nonzero regression slope, the null and alternative hypotheses are $H_0: \beta_1 = 0$ and $H_1: \beta_1 \neq 0$. Larger values of $F^\star$ tend to support the alternative hypothesis in favor of finding a regression relation. Spreading the values of $X$ is better as it provides additional opportunity for the test statistic to surpass a critical value given that a regression relationship exists. When the $X$ values are closer together, even if a regression relationship exists, it may be missed because of the deflation of the $\text{MSR}$. Given the power advantage, the original set would be more effective than this proposed set of $X$'s.
\end{bluebox}


\newpage

# Problem 4
A random sample of 100 paired observations $(X_i, Y_i)$, $i=1,2,\dots, 100$ was taken. Of the observations, 75 $(X_i,Y_i)$'s are $(2, 2)$ and 25 $(X_i,Y_i)$'s are $(1, 10)$. Suppose that we fit this data set with  a simple linear regression model $Y_i=\beta_0+\beta_1 X_i + \epsilon_i$ under the usual assumptions including Var$(\epsilon_i)=\sigma^2$.

## (a)  

What are the LS estimates $(b_0,b_1)'$ of $(\beta_0,\beta_1)'$? 

\begin{bluebox}
a
\end{bluebox}

## (b)  

What are the standard errors of $b_0$ and $b_1$, respectively? 

\begin{bluebox}
a
\end{bluebox}

## (c)  

What is the coefficient of determination $R^2$? 

\begin{bluebox}
a
\end{bluebox}

## (d)  

What is the hat matrix $H$? 

\begin{bluebox}
a
\end{bluebox}
  
## (e)  

What is an estimate of $\sigma^2$? 

\begin{bluebox}
a
\end{bluebox}


 
\newpage
# Problem 5

Work standards specify time, cost, and efficiency norms for the performance of work tasks. They are typically used to monitor job performance. In the distribution center of McCormick \& Company, Inc., data were collected to develop work standards for the time to assemble or fill customer orders. The table below contains data for a random sample of 9 orders.

\begin{center}
\begin{tabular}{c|ccccccccc}
  \hline
 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
  \hline
$X$ (number of items in the order) & 36 & 34 & 255 & 103 & 4 & 555 & 6 & 60 & 96 \\
  $Y$ (assemble time, minutes) & 27 & 15 & 71 & 35 & 8 & 60 & 3 & 10 & 10 \\
   \hline
\end{tabular}
\end{center}
```{r}
x <- c(36, 34, 255, 103, 4, 555, 6, 60, 96)
y <- c(27, 15, 71, 35, 8, 60, 3, 10, 10)
jobpf =  data.frame(x,y)
mod <- lm(formula = y ~ x)
summary(mod)
plot(x, y)
```
Using the above data of size $n=9$, answer the following questions.

## (a)  

Obtain the estimated regression function. Plot the estimated regression function and the data. Does a linear regression function appear to give a good fit here?

```{r}
ggplot(jobpf, aes(x = x, y = y)) +
  geom_point(
    size = 3,
    color = "wheat3",
    alpha = 0.8
  ) +
  geom_smooth(
    method = "lm",
    se = FALSE,
    color = "lightslateblue",
    linewidth = 1
  ) +
  labs(
    title = "Scatterplot with Linear Regression Fit",
    x = "x",
    y = "y"
  ) +
  theme_minimal(base_size = 13)
```

\begin{bluebox}

The estimated regression function is $\hat{Y} = 12.594 + 0.1094X_1$. 

Based upon the plot, the linear regression function appears to fit the data reasonably. Our regression outputs gave us reason to believe that a linear association exists ($\beta_1 \neq 0$) with the regression function providing meaningful explanatory power for the outcome variable ($R^2 \approx 0.64$). The sample size here is very small, and the perception of a linear relationship provided by the scatter plot may be subject to change under an increased sample size. While we can doubt whether this data provides enough information about the association between $X$ and $Y$, at present we do not have any reason to dispute a linear fit. 

\end{bluebox}

## (b)  

Obtain a point estimate of the expected assemble time for an order with $X = 70$ items.

\begin{bluebox}
Now, let's put our regression model to work to find out the fitted value for $X=70$:

$$
(\hat{Y}|X=70) = 12.594 + 0.1094(70) = 20.252
$$
Our point estimate for $Y$ given $X=70$ in this simple linear regression model is $20.252$.
\end{bluebox}

## (c) 

```{r}
# Calculate MSE and S_XX for CI computation:
## MSE calculation:
SSE <- sum(residuals(mod)^2)
n <- length(y)
MSE <- SSE / (n - 2)
cat("MSE:", round(MSE, 3))

## S_XX calculation:
SXX <- sum((x - mean(x))^2)
cat("S_XX:", round(SXX,3))

# Confidence interval calculation
SE_beta1 <- sqrt(MSE / SXX)

## Check if manual calculation matches model:
R_SE_beta1 <- summary(mod)$coefficients["x", "Std. Error"]
cat("Calculated SE_beta_1:", SE_beta1, "; R-given SE_beta_1:", R_SE_beta1)

## Critical t value:

crit_t <- qt(0.975, 7) # 9 data points -> n-2 = 7 df

upper_95_bound_beta_1 <- 0.1094 + crit_t*SE_beta1
lower_95_bound_beta_1 <- 0.1094 - crit_t*SE_beta1
print(paste0("95% CI for Beta_1: (", round(lower_95_bound_beta_1,3), ",", 
             round(upper_95_bound_beta_1,3), ")"))
```

Estimate $\beta_1$ with a 95 percent confidence interval. Interpret your interval estimate.

\begin{bluebox}
We can provide a $95\%$ confidence interval for $\beta_1$ by starting with our model-provided point estimate for $\beta_1$, $\hat{\beta_1} = 0.1094$ and building the outer bounds with the equation $
\hat{\beta_1} \pm t_{1-\frac{\alpha}{2}, \enspace n-2} \times \text{se} \left[ \hat{\beta_1} \right], $
where $\text{se} \left[ \hat{\beta_1} \right] = \sqrt{\frac{\text{MSE}}{S_{XX}}}$. We can calculate $S_{XX}$ and $\text{MSE}$ to provide those quantities to our confidence interval calculation, giving interval $(0.035, 0.183)$. Since we confidence interval does not contain zero, we can interpret this as evidence at the $95\%$ confidence level (in a two-sided scenario) that the regression coefficient $\beta_1$ is not equal to zero, and that therefore, a regression relationship exists between $X$ and $Y$ in this model. 

\end{bluebox}

## (d)  

Conduct a $t$ test to decide whether or not there is a linear association between number of items in an order ($X$) and the assemble time ($Y$). Use a level of significance of .05. State the alternatives, decision rule, and conclusion. What is the p-value of the test?



\begin{bluebox}
In the prior sub-question, we used confidence intervals to express the idea of identifying whether there is a linear association between $X$ and $Y$. Now, we can approach that question again from a hypothesis testing perspective. Our hypotheses will be:

$$
H_0 : \beta_1 = 0,
$$

$$
H_1 : \beta_1 \neq 0,
$$
As a slope ($\beta_1$) of $0$ would suggest no linear association. We will use a significance level $\alpha$ of $0.05$. Therefore, our decision rule will be to reject the null hypothesis which stated $\beta_1 = 0$ if we obtain a p-value of less than $0.05$ which is equivalent to rejected the null hypothesis if our test statistic exceeds $t_{0.975, \enspace 7}$ as we will use a two-sided test to mirror the construction of the confidence interval. 

The test results are automatically included in R's model summary; for $\hat{\beta_1}$, we get a t test statistic of 3.495 (exceeding the critical value) and a p-value of 0.0101, below the significance level. Therefore, we reject the null hypothesis and can conclude at a $0.05$ significance level that the linear regression coefficient for this SLR model is nonzero, and therefore, that there is a significant linear association between $X$ and $Y$ in this model. 

\end{bluebox}

## (e)  

Estimate the mean assemble time for orders with the following numbers of items: $X = 70$ and $X=100$. Use separate 99 percent confidence intervals. Interpret your results.

```{r}
# Calculate bar{x}:
x <- c(36, 34, 255, 103, 4, 555, 6, 60, 96)
print(paste0("xbar = ", round(mean(x), 2)))

# Compute Confidence Intervals:

## Critical t-value
crit_t_99ci_7df <- qt(0.995, 7)

## Fitted on X=70:
fitted_Y_x70 <- 20.252
se_hat_Y_x70 <- sqrt( 
  MSE*( (1/9) + ((mean(x) - 70)^2/SXX) )
  )
lower_bound_99ci_x70 <- fitted_Y_x70 - crit_t_99ci_7df*se_hat_Y_x70
upper_bound_99ci_x70 <- fitted_Y_x70 + crit_t_99ci_7df*se_hat_Y_x70
print(paste0("99% CI for Fitted Value of Y given X=70: (", 
             round(lower_bound_99ci_x70, 3) , ", ", 
             round(upper_bound_99ci_x70, 3) , ")"))

## Fitted on X=100: 
fitted_Y_x100 <- 23.534
se_hat_Y_x100 <- sqrt( 
  MSE*( (1/9) + ((mean(x) - 100)^2/SXX) )
  )
lower_bound_99ci_x100 <- fitted_Y_x100 - crit_t_99ci_7df*se_hat_Y_x100
upper_bound_99ci_x100 <- fitted_Y_x100 + crit_t_99ci_7df*se_hat_Y_x100
print(paste0("99% CI for Fitted Value of Y given X=100: (", 
             round(lower_bound_99ci_x100, 3), ", ", 
             round(upper_bound_99ci_x100, 3), ")"))
```

\begin{bluebox}
Now, instead of constructing a confidence interval for the linear regression coefficient, we are seeking confidence intervals for fitted values of two specific order sizes, $70$ and $100$. We will be building $99\%$ confidence intervals, which will end up being relatively wide compared to other choices of confidence intervals. We can build these confidence intervals by beginning with the fitted values and using the corresponding standard errors to expand into an interval. The CI for a fitted value in an SLR model is $(\hat{Y}|X=x) \pm t_{0.995, \enspace7} \times \sqrt{\text{MSE} \left( \frac{1}{n}+\frac{(\bar{X}-X_h)^2}{S_{XX}}\right)}$. The fitted values for $X=70$ and $X=100$ are $(\hat{Y}|X=70) = 12.594 + 0.1094(70) = 20.252$ and $(\hat{Y}|X=100) = 12.594 + 0.1094(100) = 23.534$, respectively. To compute our confidence intervals, we must know $\bar{X}$, which is $126.67$

\textbf{Confidence Interval ($99\%$) for $\hat{Y_{X=70}}$:}

The 99$\%$ confidence interval for $\hat{Y_{X=70}}$ is given by:

$$
(\hat{Y}|X=70) \pm t_{0.995, \enspace7} \times \sqrt{\text{MSE} \left( \frac{1}{9}+\frac{(126.67-70)^2}{S_{XX}}\right)} = (0.862, 39.642)
$$

\textbf{Confidence Interval ($99\%$) for $\hat{Y_{X=100}}$:}

The 99$\%$ confidence interval for $\hat{Y_{X=100}}$ is given by:

$$
(\hat{Y}|X=100) \pm t_{0.995, \enspace7} \times \sqrt{\text{MSE} \left( \frac{1}{9}+\frac{(126.67-100)^2}{S_{XX}}\right)} = (4.952, 42.116)
$$

We can interpret this confidence interval as a range of values that we expect the mean of the assembly time for these respective order sizes ($X=70,100$) to exist in at a 99$\%$ confidence level.
\end{bluebox}

## (f)  

The next order has 30 items. Obtain a 99 percent prediction interval for the assemble time for this order. Interpret your prediction interval.

```{r}
# Build prediction interval for X=30

fitted_val_30 <- predict(mod, newdata = data.frame(x = 30))
se_pred30 <- sqrt(
  MSE * (1 + (1/9) + (30-mean(x))^2/SXX)
)

lower_99_ci_bound_pred30 <- fitted_val_30 - crit_t_99ci_7df * se_pred30
upper_99_ci_bound_pred30 <- fitted_val_30 + crit_t_99ci_7df * se_pred30

print(paste0("99% Pred. Int. for Y for new obs. X=30: (", 
             round(lower_99_ci_bound_pred30, 3), ", ", 
             round(upper_99_ci_bound_pred30, 3), ")"))
```

\begin{bluebox}
The spirit of this question is similar to the last, but this time we are interested in building a \textit{prediction interval}, which is formulated in a slightly different way to a typical confidence interval for the mean response and which has a different interpretation. This time, our interval can be constructed under the structure $(\hat{\beta}_0 + \hat{\beta}_1 X_{\text{new}}) \pm t_{1-\frac{\alpha}{2},\;n-2} \sqrt{ \text{MSE} \left( 1 + \frac{1}{n} + \frac{(X_{\text{new}} - \bar{X})^2}{S_{XX}} \right)}$.

Our $99\%$ prediction interval for $X=30$ is therefore:

$$
\left(12.594 + 0.10936(30)\right) \pm t_{0.995, \; 7} \sqrt{ \text{MSE} \left( 1 + \frac{1}{9} + \frac{(30 - 126.67)^2}{S_{XX}} \right)} = (-43.059, 74.809)
$$

Since we cannot have negative time for filling an order, the interpretability of our prediction interval suffers. We obtain this negative bound because of the large interval selection, low sample size, and observed variance large enough to push the bound negative (there is no constraint for having strictly positive CI/PI bounds in SLR). We can still say with $99\%$ confidence that a future observation with $X=30$ will fall in our prediction interval. 
\end{bluebox}

## (g)  

Set up the ANOVA table. Which elements are additive?

\begin{bluebox}
a
\end{bluebox}

## (h) 

Conduct an $F$ test to decide whether or not there is a linear association between the assemble time and the number of items; control the $\alpha$ risk at .05. State the alternatives, decision rule, and conclusion.

\begin{bluebox}
a
\end{bluebox}

## (i)  

Obtain the $t^\star$ statistic for the test in part (h) and demonstrate numerically its equivalence to the $F^\star$ statistic obtained in part (h).

\begin{bluebox}
a
\end{bluebox}

## (j)  

Calculate $R^2$ and $r$. What proportion of the variation in $Y$ is accounted for by introducing $X$ into the regression model?

\begin{bluebox}
a
\end{bluebox}

## (k) 

It took 50 minutes to assemble an order. Obtain a 95 percent confidence interval for the number of items in this order using both the delta and exact methods. Interpret your confidence interval.

\begin{bluebox}
a
\end{bluebox}

## (l) 

In part (k), is criterion in page 52 of Note 3 satisfied regarding the appropriateness of the approximate confidence interval?

\begin{bluebox}
a
\end{bluebox}

\newpage

# Problem 6 

In fitting a simple linear regression on a data set with mean response $\bar{Y}=0.9112$ and covariate $X$ ranging from -2.224 to  2.441, the following partial R outputs are obtained.

\begin{verbatim}
Call:
lm(formula = y ~ x)
Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)   0.9026     0.1926
x             0.3776     0.2192

Analysis of Variance Table
Response: y
          Df Sum Sq Mean Sq F value  Pr(>F)
x          1   11.0
Residuals 98

Residual standard error: 1.925 on 98 degrees of freedom
\end{verbatim}

## (a) 

Find the sample size $n$, SSE, $\hat\sigma^2$, and R-square $R^2$.
  
\begin{bluebox}
a
\end{bluebox}
  
## (b)  

Find the p-value for testing $H_0:\beta_0=0$ versus $H_1:\beta_0\neq0$, and the p-value for testing $H_0:\beta_1=0$ versus $H_1:\beta_1 < 0$.

\begin{bluebox}
a
\end{bluebox}

## (c)  

Find the sample coefficient of variation for the covariate $X$, CV$_X$ (see page 45 of Note 1 for the definition).
  

  For the same data set, we have the following
\begin{verbatim}
> CI<-predict(fit, se.fit=TRUE, interval = c("confidence"))
> head(cbind(y,CI$fit))
            y       fit        lwr      upr
1  2.69001380 1.3794827  0.7183901 2.040575
2 -0.58617658 0.7794220  0.3683444 1.190500
3 -0.09899989 1.4047227  0.7196928 2.089753
\end{verbatim}
 
\begin{bluebox}
a
\end{bluebox}

## (d)  

For $X$ with the same value as in the first observation (the first line of the output with $Y=2.69001380$), find the SE of estimating the mean response and the SE of $Y_{new} -\hat{Y}_{new}$.

\begin{bluebox}
a
\end{bluebox}

## (e) 

For a single observation with $X=1$, find the 95\% prediction interval for the response.

\begin{bluebox}
a
\end{bluebox}
           